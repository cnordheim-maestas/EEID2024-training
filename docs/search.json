[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "EEID 2024 workshop Schedule",
    "section": "",
    "text": "The EEID 2024 workshop session Estimating environmental response functions using Bayesian inference will be developed on three sesions from 22nd June to 24th June of 2024. The sessions include both theoretical input and practical sessions. Practical sessions will take a relevant part of the time on which you will be developing exercises and activities. In addition, it is expected for each participant to develop a small project which will present at the end of the workshop. It is worth to keep in mind that working outside the synchronous sessions is suggested to achieve the objectives of the workshop given its schedule.\nYou can access to activities, lectures, and readings in materials."
  },
  {
    "objectID": "schedule.html#day-1-22nd-june-2024",
    "href": "schedule.html#day-1-22nd-june-2024",
    "title": "EEID 2024 workshop Schedule",
    "section": "Day 1 (22nd June 2024)",
    "text": "Day 1 (22nd June 2024)\n\n\n\nTime\nActivity\nMaterial\n\n\n\n\n09:00\nIntroductions & course overview\n\n\n\n09:30\nIntroduction to traits and disease modeling\nslides\n\n\n10:00\nGoal setting\n\n\n\n10:15\nLightning introductions and goals\n\n\n\n10:35\nBreak\n\n\n\n11:00\nIntroducing tools: Downloading and installing bayesTPC, VecTraits database\nslides\n\n\n11:40\nPractice: Visualizing traits data\nPractice\n\n\n12:00\nEnd of day 1"
  },
  {
    "objectID": "schedule.html#day-2-23rd-june-2024",
    "href": "schedule.html#day-2-23rd-june-2024",
    "title": "EEID 2024 workshop Schedule",
    "section": "Day 2 (23rd June 2024)",
    "text": "Day 2 (23rd June 2024)\n\n\n\nTime\nActivity\nMaterial\n\n\n\n\n12:30\nIntroductions to Bayes\nPractice\n\n\n13:45\nBreak\n\n\n\n14:00\nBayesian computation\nPractice\n\n\n15:15\nBreak\n\n\n\n15:30\nUsing bayesTPC (practical)\nPractice\n\n\n16:45\nProposing project analysis\n\n\n\n17:10\nLightning presentations of project ideas\n\n\n\n17:30\nEnd of session"
  },
  {
    "objectID": "schedule.html#day-3-24th-june-2024",
    "href": "schedule.html#day-3-24th-june-2024",
    "title": "EEID 2024 workshop Schedule",
    "section": "Day 3 (24th June 2024)",
    "text": "Day 3 (24th June 2024)\n\n\n\nTime\nActivity\nMaterial\n\n\n\n\n09:00\nAdvance topic using bayesTPC\n\n\n\n10:00\nBreak\n\n\n\n10:15\nWork on project analysis\n\n\n\n11:15\nPreparing a follow-up slide\n\n\n\n11:30\nProject presentation\n\n\n\n12:00\nEnd of session"
  },
  {
    "objectID": "VB_Bayes1.html#learning-objectives",
    "href": "VB_Bayes1.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the basic principles underlying Bayesian modeling methodology\nIntroduce how to use Bayesian inference for real-world problems\nIntroduce computation tools to perform inference for simple models in R (how to turn the Bayesian crank)"
  },
  {
    "objectID": "VB_Bayes1.html#what-is-bayesian-inference",
    "href": "VB_Bayes1.html#what-is-bayesian-inference",
    "title": "VectorByte Methods Training",
    "section": "What is Bayesian Inference?",
    "text": "What is Bayesian Inference?\nIn the Bayesian approach our probabilities numerically represent rational beliefs.\n\nBayes rule provides a rational method for updating those beliefs in light of new information and incorporating/quantifying uncertainty in those beliefs.\n\nThus, Bayesian inference is an approach for understanding data inductively."
  },
  {
    "objectID": "VB_Bayes1.html#recall-bayes-theorem",
    "href": "VB_Bayes1.html#recall-bayes-theorem",
    "title": "VectorByte Methods Training",
    "section": "Recall: Bayes Theorem",
    "text": "Recall: Bayes Theorem\nBayes Theorem allows us to relate the conditional probabilities of two events \\(A\\) and \\(B\\): \\[\n\\text{Pr}(A|B) = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#what-is-bayesian-inference-1",
    "href": "VB_Bayes1.html#what-is-bayesian-inference-1",
    "title": "VectorByte Methods Training",
    "section": "What is Bayesian Inference?",
    "text": "What is Bayesian Inference?\nWe can re-write Bayes rule in terms of our parameters, \\(\\theta\\) and our data, \\(Y\\): \\[\\begin{align*}\n\\text{Pr}(\\theta|Y) & = \\frac{\\text{Pr}(Y|\\theta)\\text{Pr}(\\theta)}{\\text{Pr}(Y)}\n\\end{align*}\\]\nThe LHS is the main quantity of interest in a Bayesian analysis, the posterior, denoted \\(f(\\theta|Y)\\): \\[\n\\overbrace{f(\\theta|Y)}^\\text{Posterior} \\propto \\overbrace{\\mathcal{L}(\\theta; Y)}^\\text{Likelihood} \\times \\overbrace{f(\\theta)}^\\text{Prior}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#bayesian-methods-provide",
    "href": "VB_Bayes1.html#bayesian-methods-provide",
    "title": "VectorByte Methods Training",
    "section": "Bayesian methods provide",
    "text": "Bayesian methods provide\n\nmodels for rational, quantitative learning\nparameter estimates with good statistical properties\nestimators that work for small and large sample sizes\nparsimonious descriptions of data, predictions for missing data, and forecasts for future data\na coherent computational framework for model estimation, selection and validation"
  },
  {
    "objectID": "VB_Bayes1.html#classical-vs-bayesian",
    "href": "VB_Bayes1.html#classical-vs-bayesian",
    "title": "VectorByte Methods Training",
    "section": "Classical vs Bayesian",
    "text": "Classical vs Bayesian\nThe fundamental differences between classical and Bayesian methods is what is fixed and what is random in an analysis.\n\n\n\n\nParadigm\nFixed\nRandom\n\n\n\n\nClassical\nparam (\\(\\theta\\))\ndata (\\(Y\\))\n\n\nBayesian\ndata (\\(Y\\))\nparam (\\(\\theta\\))"
  },
  {
    "objectID": "VB_Bayes1.html#whywhy-not-bayesian-statistics",
    "href": "VB_Bayes1.html#whywhy-not-bayesian-statistics",
    "title": "VectorByte Methods Training",
    "section": "Why/Why Not Bayesian Statistics?",
    "text": "Why/Why Not Bayesian Statistics?\n\nPros\n\nIf \\(f(\\theta)\\) & \\(\\mathcal{L}(\\theta; Y)\\) represent a rational person’s beliefs, then Bayes’ rule is an optimal method of updating these beliefs given new info (Cox 1946, 1961; Savage 1954; 1972).\nProvides more intuitive answers in terms of the probability that parameters have particular values.\nIn many complicated statistical problems there are no obvious non-Bayesian inference methods."
  },
  {
    "objectID": "VB_Bayes1.html#whywhy-not-bayesian-statistics-1",
    "href": "VB_Bayes1.html#whywhy-not-bayesian-statistics-1",
    "title": "VectorByte Methods Training",
    "section": "Why/Why Not Bayesian Statistics?",
    "text": "Why/Why Not Bayesian Statistics?\n\nCons\n\nIt can be hard to mathematically formulate prior beliefs (choice of \\(f(\\theta)\\) often ad hoc or for computational reasons).\nPosterior distributions can be sensitive to prior choice.\nAnalyses can be computationally costly."
  },
  {
    "objectID": "VB_Bayes1.html#steps-to-making-inference",
    "href": "VB_Bayes1.html#steps-to-making-inference",
    "title": "VectorByte Methods Training",
    "section": "Steps to Making Inference",
    "text": "Steps to Making Inference\n\nResearch question\nData collection\nModel \\(Y_i \\approx f(X_i)\\)\nEstimate the parameter in the model with uncertainty\nMake inference\n\nThe difference between Classical and Bayesian lies in step 4:\n\nClassical uses maximum likelihood estimatation\n\nBayesian derives a posterior distribution."
  },
  {
    "objectID": "VB_Bayes1.html#example-estimating-the-probability-of-a-rare-event",
    "href": "VB_Bayes1.html#example-estimating-the-probability-of-a-rare-event",
    "title": "VectorByte Methods Training",
    "section": "Example: Estimating the probability of a rare event",
    "text": "Example: Estimating the probability of a rare event\nSuppose we are interested in the prevalence of an infectious disease in a small city. A small random sample of 20 individuals will be checked for infection.\n\nInterest is in the fraction of infected individuals \\[\n\\theta \\in \\Theta =[0,1]\n\\]\nThe data records the number of infected individuals \\[\ny \\in \\mathcal{Y} =\\{0,1, \\ldots, 20\\}\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-likelihoodsampling-model",
    "href": "VB_Bayes1.html#example-likelihoodsampling-model",
    "title": "VectorByte Methods Training",
    "section": "Example: Likelihood/sampling model",
    "text": "Example: Likelihood/sampling model\nBefore the sample is obtained, the number of infected individuals is unknown.\n\nLet \\(Y\\) denote this to-be-determined value\nIf \\(\\theta\\) were known, a sensible sampling model is \\[\nY|\\theta \\sim  \\mathrm{Bin} (20, \\theta)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-prior",
    "href": "VB_Bayes1.html#example-prior",
    "title": "VectorByte Methods Training",
    "section": "Example: Prior",
    "text": "Example: Prior\nOther studies from various parts of the country indicate that the infection rate ranges from about 0.05 to 0.20, with an average prevalence of 0.1.\n\nMoment matching from a beta distribution (a convenient choice) gives the prior \\(\\theta \\sim \\mathrm{Beta} (2,20)\\)"
  },
  {
    "objectID": "VB_Bayes1.html#example-posterior",
    "href": "VB_Bayes1.html#example-posterior",
    "title": "VectorByte Methods Training",
    "section": "Example: Posterior",
    "text": "Example: Posterior\nThe prior and sample model combination: \\[\\begin{align*}\n\\theta & \\sim  \\mathrm{Beta} (a,b) \\\\\nY|\\theta &  \\sim  \\mathrm{Bin} (n, \\theta)\n\\end{align*}\\] and an observed \\(y\\) (the data), leads to the posterior \\[\np(\\theta|y)= \\mathrm{Beta}(a+y, b+n-y)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-posterior-1",
    "href": "VB_Bayes1.html#example-posterior-1",
    "title": "VectorByte Methods Training",
    "section": "Example: Posterior",
    "text": "Example: Posterior\nFor our case, we have \\(a=2\\), \\(b=20\\), \\(n=20\\).\nIf we don’t find any infections (\\(y=0\\)) our posterior is: \\[\np(\\theta |y=0)= \\mathrm{Beta}(2, 40)\n\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-sensitivity-analysis",
    "href": "VB_Bayes1.html#example-sensitivity-analysis",
    "title": "VectorByte Methods Training",
    "section": "Example: Sensitivity Analysis",
    "text": "Example: Sensitivity Analysis\nHow influential is our prior? The posterior expectation is \\[\n\\mathrm{E}\\{\\theta|Y=y\\} =   \\frac{n}{w+n} \\bar{y} + \\frac{w}{w+n} \\theta_0\n\\] a weighted average of the sample mean and the prior expectation: \\[\\begin{align*}\n\\theta_0 & =  \\frac{a}{a+b} ~~~~ \\rightarrow \\text{ prior expectation (or guess)} \\\\\nw & = a + b  ~~~~ \\rightarrow \\text{  prior confidence}\n\\end{align*}\\]"
  },
  {
    "objectID": "VB_Bayes1.html#example-a-non-bayesian-approach",
    "href": "VB_Bayes1.html#example-a-non-bayesian-approach",
    "title": "VectorByte Methods Training",
    "section": "Example: A non-Bayesian approach",
    "text": "Example: A non-Bayesian approach\nA standard estimate of a population proportion, \\(\\theta\\) is the sample mean \\(\\bar{y} = y/n\\). If \\(y=0 \\rightarrow \\bar{y} = 0\\).\n\nUnderstanding the sampling uncertainty is crucial (e.g., for reporting to health officials).\nThe most popular 95% confidence interval for a population proportion is the Wald Interval: \\[\n\\bar{y} \\pm 1.96 \\sqrt{\\bar{y}(1-\\bar{y})/n}.\n\\] This has the correct asymptotic coverage, but \\(y=0\\) is still problematic!"
  },
  {
    "objectID": "VB_Bayes1.html#conjugate-bayesian-models",
    "href": "VB_Bayes1.html#conjugate-bayesian-models",
    "title": "VectorByte Methods Training",
    "section": "Conjugate Bayesian Models",
    "text": "Conjugate Bayesian Models\nSome sets of priors/likelihoods/posteriors exhibit a special relationship called conjugacy: when posterior and prior distributions have the same form.\nE.g., in our Beta-Binomial/Bernoilli example: \\[\\begin{align*}\n\\theta & \\sim  \\mathrm{Beta} (a,b) \\\\\nY|\\theta &  \\sim  \\mathrm{Bin} (n, \\theta) \\\\\n\\theta | Y & \\sim  \\mathrm{Beta}(a^*, b^*)\n\\end{align*}\\]"
  },
  {
    "objectID": "VB_Bayes1.html#are-all-posteriors-in-the-same-family-as-the-priors-no",
    "href": "VB_Bayes1.html#are-all-posteriors-in-the-same-family-as-the-priors-no",
    "title": "VectorByte Methods Training",
    "section": "Are all posteriors in the same family as the priors? No",
    "text": "Are all posteriors in the same family as the priors? No\n\nConjugacy is a nice special property, but most of the time this isn’t the case.\n\nUsually getting an analytic form of the posterior distribution can be hard or impossible."
  },
  {
    "objectID": "VB_Bayes1.html#what-do-you-do-with-a-posterior",
    "href": "VB_Bayes1.html#what-do-you-do-with-a-posterior",
    "title": "VectorByte Methods Training",
    "section": "What do you do with a Posterior?",
    "text": "What do you do with a Posterior?\n\nSummarize important aspects of the posterior\n\nmean, median, mode, variance…\n\nCheck sensitivity of posterior to prior choice\nSay what range of parameters is consistent with the observed data given our prior information\nMake predictions"
  },
  {
    "objectID": "VB_Bayes1.html#posterior-summaries-point",
    "href": "VB_Bayes1.html#posterior-summaries-point",
    "title": "VectorByte Methods Training",
    "section": "Posterior Summaries (point)",
    "text": "Posterior Summaries (point)\nFor the Beta-Binomial model, we found that \\[\np(\\theta | y)= \\mathrm{Beta}(a+y, b+n-y).\n\\] We can calculate multiple summaries exactly, for example: \\[\\begin{align*}\n\\mathrm{mean}= \\mathrm{E}[\\theta|Y] & = \\frac{a+y}{a+b+n} \\\\\n\\mathrm{mode}(\\theta|Y) & = \\frac{a+y-1}{a+b+n-2} ~~~ \\dagger  \n\\end{align*}\\]\n \\(\\dagger\\) a.k.a. the maximum a posteriori estimator (MAP)"
  },
  {
    "objectID": "VB_Bayes1.html#prior-sensitivity",
    "href": "VB_Bayes1.html#prior-sensitivity",
    "title": "VectorByte Methods Training",
    "section": "Prior Sensitivity",
    "text": "Prior Sensitivity\nThe posterior expectation can be written as a weighted average of information from the prior and the data \\[\n\\mathrm{E}\\{\\theta |Y=y\\} =   \\frac{n}{a + b +n} \\bar{y} + \\frac{a+b}{a+b+n} \\theta_0.\n\\] Thus \\(a\\) and \\(b\\) can be interpreted here as prior data where \\(a\\) is the number of prior successes and \\(a+b\\) is the prior sample size. When \\(n\\gg a+b\\) most of our information comes from the data instead of the prior."
  },
  {
    "objectID": "VB_Bayes1.html#visualizing-the-prior-vs.-posterior",
    "href": "VB_Bayes1.html#visualizing-the-prior-vs.-posterior",
    "title": "VectorByte Methods Training",
    "section": "Visualizing the prior vs. posterior",
    "text": "Visualizing the prior vs. posterior\nWe can also visually check for sensitivity, since we don’t have general analytic approaches."
  },
  {
    "objectID": "VB_Bayes1.html#confidence-regions",
    "href": "VB_Bayes1.html#confidence-regions",
    "title": "VectorByte Methods Training",
    "section": "Confidence Regions",
    "text": "Confidence Regions\nAn interval \\([l(y), u(y)]\\), based on the observed data \\(Y=y\\), has 95 % Bayesian coverage for \\(\\theta\\) if \\[\nP(l(y) &lt;\\theta &lt; u(y)|Y=y)=0.95\n\\] The interpretation: it describes your information about the true value of \\(\\theta\\) after you have observed \\(Y=y\\).\n Such intervals are typically called credible intervals, to distinguish them from frequentist confidence intervals. Both are referred to as CIs."
  },
  {
    "objectID": "VB_Bayes1.html#quantile-based-bayesian-ci",
    "href": "VB_Bayes1.html#quantile-based-bayesian-ci",
    "title": "VectorByte Methods Training",
    "section": "Quantile-based (Bayesian) CI",
    "text": "Quantile-based (Bayesian) CI\nPerhaps the easiest way to obtain a credible interval is to use the posterior quantiles.\nTo make a \\(100 \\times (1-\\alpha)\\) % quantile-based CI, find numbers \\(\\theta_{\\alpha/2}&lt;\\theta_{1- \\alpha/2}\\) such that\n\n\\(P(\\theta &lt;\\theta_{\\alpha/2} |Y=y)=\\alpha/2\\)\n\\(P(\\theta &gt;\\theta_{1-\\alpha/2} |Y=y)=\\alpha/2\\)\n\nThe numbers \\(\\theta_{\\alpha/2},\\theta_{1- \\alpha/2}\\) are the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) posterior quantiles of \\(\\theta\\)."
  },
  {
    "objectID": "VB_Bayes1.html#example-binomial-sampling-uniform-prior",
    "href": "VB_Bayes1.html#example-binomial-sampling-uniform-prior",
    "title": "VectorByte Methods Training",
    "section": "Example: Binomial sampling + uniform prior",
    "text": "Example: Binomial sampling + uniform prior\nSuppose out of \\(n=10\\) conditionally independent draws of a binary random variable we observe \\(Y=2\\) ones (successes).\n Using a uniform prior distribution (a.k.a., \\(\\mathrm{Beta}(1,1)\\)) for \\(\\theta\\), the posterior distribution is \\(\\theta | y=2 \\sim \\mathrm{Beta}(1+2,1+10-2)\\)."
  },
  {
    "objectID": "VB_Bayes1.html#alternative-hpd-region",
    "href": "VB_Bayes1.html#alternative-hpd-region",
    "title": "VectorByte Methods Training",
    "section": "Alternative: HPD region",
    "text": "Alternative: HPD region\nA \\(100 \\times(1-\\alpha)\\) % highest posterior density (HPD) regions is the part of parameter space, \\(s(y)\\), such that:\n\n\\(P(\\theta \\in s(y) |Y=y)= 1-\\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\notin s(y)\\) then \\(P(\\theta_a |Y=y)&gt;P(\\theta_b |Y=y)\\)\n\n\\(\\Rightarrow\\) all points inside the HPD region have higher probability density than those outside."
  },
  {
    "objectID": "VB_Bayes1.html#next-steps",
    "href": "VB_Bayes1.html#next-steps",
    "title": "VectorByte Methods Training",
    "section": "Next Steps",
    "text": "Next Steps\nNext you’ll complete a practical where you conduct a conjugate Bayesian analysis for the mean of a normal distribution on the midge data introduced in the likelihood chapter. You’ll also visualize the effect of different prior choices on the posterior distribution."
  },
  {
    "objectID": "Stats_review.html",
    "href": "Stats_review.html",
    "title": "VectorByte Methods Training 2023",
    "section": "",
    "text": "Main materials\nSolutions to exercises"
  },
  {
    "objectID": "Stats_review.html#some-probability-notation",
    "href": "Stats_review.html#some-probability-notation",
    "title": "VectorByte Methods Training 2023",
    "section": "Some probability notation",
    "text": "Some probability notation\nWe have a set, S of all possible events. Let \\text{Pr}(A) (or alternatively \\text{Prob}(A)) be the probability of event A. Then:\n\nA^c is the complement to A (all events that are not A).\nA \\cup B is the union of events A and B (“A or B”).\nA \\cap B is the intersection of events A and B (“A and B”).\n\\text{Pr}(A|B) is the conditional probability of A given that B occurs."
  },
  {
    "objectID": "Stats_review.html#axioms-of-probability",
    "href": "Stats_review.html#axioms-of-probability",
    "title": "VectorByte Methods Training 2023",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\nThese are the basic definitions that we use when we talk about probabilities. You’ve probably seen these before, but maybe not in mathematical notation. If the notation is new to you, I suggest that you use the notation above to translate these statements into words and confirm that you understand what they mean. I give you an example for the first statement.\n\n\\sum_{i \\in S} \\text{Pr}(A_i)=1, where 0 \\leq \\text{Pr}(A_i) \\leq 1 (the probabilities of all the events that can happen must sum to one, and all of the individual probabilities must be less than one)\n\\text{Pr}(A)=1-\\text{Pr}(A^c)\n\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) -\\text{Pr}(A \\cap B)\n\\text{Pr}(A \\cap B) = \\text{Pr}(A|B)\\text{Pr}(B)\nIf A and B are independent, then \\text{Pr}(A|B) = \\text{Pr}(A)"
  },
  {
    "objectID": "Stats_review.html#bayes-theorem",
    "href": "Stats_review.html#bayes-theorem",
    "title": "VectorByte Methods Training 2023",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes Theorem allows us to related the conditional probabilities of two events A and B:\n\\begin{align*}\n\\text{Pr}(A|B) & = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\\\\n&\\\\\n& =  \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B|A)\\text{Pr}(A) + \\text{Pr}(B|A^c)\\text{Pr}(A^c)}\n\\end{align*}"
  },
  {
    "objectID": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Discrete RVs and their Probability Distributions",
    "text": "Discrete RVs and their Probability Distributions\nMany things that we observe are naturally discrete. For instance, whole numbers of chairs or win/loss outcomes for games. Discrete probability distributions are used to describe these kinds of events.\nFor discrete RVs, the distribution of probabilities is described by the probability mass function (pmf), f_k such that:\n\\begin{align*}\nf_k  \\equiv \\text{Pr}(X & = k) \\\\\n\\text{where } 0\\leq f_k \\leq 1 & \\text{ and } \\sum_k f_k = 1\n\\end{align*}\nFor example, for a fair 6-sided die:\nf_k = 1/6 for k= \\{1,2,3,4,5,6\\}.\n\\star Question 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\nRelated to the pmf is the cumulative distribution function (cdf), F(x). F(x) \\equiv \\text{Pr}(X \\leq x)\nFor the 6-sided die F(x)= \\displaystyle\\sum_{k=1}^{x} f_k\nwhere x \\in 1\\dots 6.\n\\star Question 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\n\nVisualizing distributions of discrete RVs in R\nExample: Imagine a RV can take values 1 through 10, each with probability 0.1:\n \n\nvals&lt;-seq(1,10, by=1)\npmf&lt;-rep(0.1, 10)\ncdf&lt;-pmf[1]\nfor(i in 2:10) cdf&lt;-c(cdf, cdf[i-1]+pmf[i])\npar(mfrow=c(1,2), bty=\"n\")\nbarplot(height=pmf, names.arg=vals, ylim=c(0, 1), main=\"pmf\", col=\"blue\")\nbarplot(height=cdf, names.arg=vals, ylim=c(0, 1), main=\"cdf\", col=\"red\")"
  },
  {
    "objectID": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training 2023",
    "section": "Continuous RVs and their Probability Distributions",
    "text": "Continuous RVs and their Probability Distributions\nThings are just a little different for continuous RVs. Instead we use the probability density function (pdf) of the RV, and denote it by f(x). It still describes how relatively likely are alternative values of an RV – that is, if the pdf his higher around one value than around another, then the first is more likely to happen. However, the pdf does not return a probability, it is a function that describes the probability density.\nAn analogy:\nProbabilities are like weights of objects. The PMF tells you how much weight each possible value or outcome contributes to a whole. The PDF tells you how dense it is around a value. To calculate the weight of a real object, you need to also know the size of the area that you’re interested in and the density there The probability that your RV takes exactly any value is zero, just like the probability that any atom in a very thin wire is lined up at exactly that position is zero (and to the amount of mass at that location is zero). However, you can take a very thin slice around that location to see how much material is there.\nRelated to the pdf is the cumulative distribution function (cdf), F(x). \nF(x) \\equiv \\text{Pr}(X \\leq x)\n For a continuous distribution: \nF(x)= \\int_{-\\infty}^x f(x')dx'\n\n \n For a normal distribution with mean 0, what is F(0)?\n \n\nVisualizing distributions of continuous RVs in R\nExample: exponential RV, where f(x) = re^{-rx}:\n\n\nvals&lt;-seq(0,10, length=1000)\nr&lt;-0.5\npar(mfrow=c(1,2), bty=\"n\")\nplot(vals, dexp(vals, rate=r), main=\"pdf\", col=\"blue\", type=\"l\", lwd=3, ylab=\"\", xlab=\"\")\nplot(vals, pexp(vals, rate=r), main=\"cdf\", ylim=c(0,1), col=\"red\",\n     type=\"l\", lwd=3, ylab=\"\", xlab=\"\")"
  },
  {
    "objectID": "Stats_review.html#confidence-intervals",
    "href": "Stats_review.html#confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSuppose Z_{n-p} \\sim t_{n-p}(0,1). A centered interval is on this t distribution can be written as: \\text{Pr}(-t_{n-p,\\alpha/2} \\&lt; Z\\_{n-p} \\&lt; t_{n-p,\\alpha/2}) = 1-\\alpha. That is, between these values of the t distribution (1-\\alpha)\\times 100 percent of the probability is contained in that symmetric interval. We can visually indicate these location on a plot of the t distribution (here with df=5 and \\alpha=0.05):\n\nx&lt;-seq(-4.5, 4.5, length=1000)\nalpha=0.05\n\n## draw a line showing the normal pdf on the histogram\nplot(x, dt(x, df=5), col=\"black\", lwd=2, type=\"l\", xlab=\"x\", ylab=\"\")\nabline(v=qt(alpha/2, df=5), col=3, lty=2, lwd=2)\nabline(v=qt(1-alpha/2, df=5), col=2, lty=2, lwd=2)\n\nlegend(\"topright\", \n       legend=c(\"t, df=5\", \"lower a/2\", \"upper a/2\"),\n       col=c(1,3,2), lwd=2, lty=c(1, 2,2))\n\n\n\n\n\n\n\n\nIn the R code here, {\\tt qt} is the Student-t “quantile function”. The function {\\tt qt(alpha, df)} returns a value z such that \\alpha = P(Z_{\\mathrm{df}} &lt; z), i.e., t_{\\mathrm{df},\\alpha}.\nHow can we use this to determine the confidence interval for \\theta? Since \\theta \\sim t_{n-p}(\\mu, s^2), we can replace the Z_{n-p} in the interval above with the definition in terms of \\theta, \\mu and s and rearrange: \\begin{align*}\n1-\\alpha& = \\text{Pr}\\left(-t_{n-p,\\alpha/2} &lt; \\frac{\\mu - \\bar{\\theta}}{s} &lt;\nt_{n-p,\\alpha/2}\\right) \\\\\n&=\n\\text{Pr}(\\bar{\\theta}-t_{n-p,\\alpha/2}s &lt; \\mu &lt;\n\\bar{\\theta} + t_{n-p,\\alpha/2}s)\n\\end{align*}\nThus (1-\\alpha)*100% of the time, \\mu is within the confidence interval (written in two equivalent ways):\n\\bar{\\theta} \\pm t_{n-p,\\alpha/2} \\times s \\;\\;\\; \\Leftrightarrow \\;\\;\\; \\bar{\\theta}-t_{n-p,\\alpha/2} \\times s, \\bar{\\theta} + t_{n-p,\\alpha/2}\\times s\nWhy should we care about confidence intervals?\n\nThe confidence interval captures the amount of information in the data about the parameter.\nThe center of the interval tells you what your estimate is.\nThe length of the interval tells you how sure you are about your estimate."
  },
  {
    "objectID": "Stats_review.html#p-values",
    "href": "Stats_review.html#p-values",
    "title": "VectorByte Methods Training 2023",
    "section": "p-Values",
    "text": "p-Values\nWhat is a p-value? The American Statistical Association issued a statement where they defined it in the following way:\n“Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (ASA Statement on Statistical Significance and P-Values.)\nMore formally, we formulate a p-value in terms of a null hypothesis/model and test whether or not our observed data are more extreme than we would expect under that specific null model. In your previous courses you’ve probably seen very specific null models, corresponding to, for instance the null hypothesis that the mean of your data is normally distributed with mean m (often m=0). We often denote the null model as H_0 and the alternative as H_a or H_1. For instance, for our example above with \\theta we might want to test the following:\nH_0: \\bar{\\theta}=0 \\;\\;\\; \\text{vs.} \\;\\;\\; H_a: \\bar{\\theta}\\neq 0\nTo perform the hypothesis test we would FIRST choose our rejection level, \\alpha. Although convention is to use \\alpha =0.05 corresponding to a 95% confidence region, one could choose based on how sure one needs to be for a particular application. Next we build our test statistic. There are two cases, first if we know \\sigma and second if we don’t.\nIf we knew the variance \\sigma^2, our test statistic would be Z=\\frac{\\bar{\\theta}-0}{\\sigma}, and we expect that this should have a standard normal distribution, i.e., Z\\sim\\mathcal{N}(0,1). If we don’t know \\sigma and instead estimate is as s (which is most of the time), our test statistic would be Z_{df}=\\frac{\\bar{\\theta}-0}{s} (i.e., it would have a t-distribution).\nWe calculate the value of the appropriate statistic (either Z or Z_{df}) for our data, and then we compare it to the values of the standard distribution (normal or t, respectively) corresponding to the \\alpha level that we chose, i.e., we see if the number that we got for our statistic is inside the horizontal lines that we drew on the standard distribution above. If it is, then the data are consistent with the null hypothesis and we cannot reject the null. If the statistic is outside the region the data are NOT consistent with the null, and instead we reject the null and use the alternative as our new working hypothesis.\nNotice that this process is focused on the null hypothesis. We cannot tell if the alternative hypothesis is true, or, really, if it’s actually better than the null. We can only say that the null is not consistent with our data (i.e., we can falsify the null) at a given level of certainty.\nAlso, the hypothesis testing process is the same as building a confidence interval, as above, and then seeing if the null hypothesis is within your confidence interval. If the null is outside of your confidence interval then you can reject your null at the level of certainty corresponding to the \\alpha that you used to build your CI. If the value for the null is within your CI, you cannot reject at that level."
  },
  {
    "objectID": "Stats_review.html#the-sampling-distribution-1",
    "href": "Stats_review.html#the-sampling-distribution-1",
    "title": "VectorByte Methods Training 2023",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,9) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\nWhat is the expectation of the sample mean?\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\nWhat is the standard error of \\bar{Y}?"
  },
  {
    "objectID": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "href": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "title": "VectorByte Methods Training 2023",
    "section": "Hypothesis Testing and Confidence Intervals",
    "text": "Hypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq 12, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?"
  },
  {
    "objectID": "Stats_review_soln.html",
    "href": "Stats_review_soln.html",
    "title": "VectorByte Methods Training 2023",
    "section": "",
    "text": "Main materials\nBack to stats review\n\nQuestion 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\n\nAnswer: both of these are zero, because the die cannot take these values.\n    \n\n\nQuestion 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\nAnswer: The CDF total probability of having a value less than or equal to its argument. Thus F(3)= 1/2, F(7)=1, and F(1.5)=1/6\n    \n\n\nQuestion 3: For a normal distribution with mean 0, what is F(0)?\n\nAnswer: The normal distribution is symmetric around its mean, with half of its probability on each side. Thus, F(0)=1/2\n    \n\n\nQuestion 4: Summation Notation Practice\n\n\n\ni\n1\n2\n3\n4\n\n\n\n\nZ_i\n2.0\n-2.0\n3.0\n-3.0\n\n\n\n\nCompute \\sum_{i=1}^{4}{z_i} = 0 \nCompute \\sum_{i=1}^4{(z_i - \\bar{z})^2} = 26 \nWhat is the sample variance? Assume that the z_i are i.i.d.. Note that i.i.d.~stands for “independent and identically distributed”. \n\nSolution: \ns^2= \\frac{\\sum_{i=1}^N(Y_i - \\bar{Y})^2}{N-1} = \\frac{26}{3}\n= 8\\times \\frac{2}{3}\n \n\nFor a general set of N numbers, \\{X_1, X_2, \\dots, X_N \\} and \\{Y_1, Y_2, \\dots, Y_N \\} show that \n\\sum_{i=1}^N{(X_i - \\bar{X})(Y_i - \\bar{Y})} = \\sum_{i=1}^N{(X_i-\\bar{X})Y_i}\n\n\n Solution: First, we multiply through and distribute: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\sum_{i=1}^N(X_i-\\bar{X})\\bar{Y}\n Next note that \\bar{Y} (the mean of the Y_is) doesn’t depend on i so we can pull it out of the summation: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\bar{Y} \\sum_{i=1}^N(X_i-\\bar{X}).\n Finally, the last sum must be zero because \n\\sum_{i=1}^N(X_i-\\bar{X}) = \\sum_{i=1}^N X_i- \\sum_{i=1}^N \\bar{X} = N\\bar{X} - N\\bar{X}=0.\n Thus \\begin{align*}\n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum_{i=1}^N(X_i-\\bar{X})Y_i - \\bar{Y}\\times 0\\\\\n& = \\sum_{i=1}^N(X_i-\\bar{X})Y_i.\n\\end{align*}\n    \n\n\nQuestion 5: Properites of Expected Values\nUsing the definition of an expected value above and with X and Y having the same probability distribution, show that:\n\\begin{align*}\n\\text{E}[X+Y]  & = \\text{E}[X] + \\text{E}[Y]\\\\  \n& \\text{and} \\\\\n\\text{E}[cX]  & = c\\text{E}[X]. \\\\\n\\end{align*}\nGiven these, and the fact that \\mu=\\text{E}[X], show that:\n\\begin{align*}\n\\text{E}[(X-\\mu)^2]  = \\text{E}[X^2] - (\\text{E}[X])^2\n\\end{align*}\nThis gives a formula for calculating variances (since \\text{Var}(X)= \\text{E}[(X-\\mu)^2]).\nSolution: Assuming X and Y are both i.i.d. with distribution f(x). The expectation of X+Y is defined as \\begin{align*}\n\\text{E}[X+Y]  & =  \\int (X+Y) f(x)dx \\\\\n              & =  \\int (X f(x) +Y f(x))dx  \\\\\n              & =  \\int X f(x)dx  +\\int Y f(x)dx  \\\\\n               & = \\text{E}[X] + \\text{E}[Y]  \n\\end{align*} Similarly \\begin{align*}\n\\text{E}[cX]   & =  \\int cXf(x)dx \\\\\n              & =  c \\int Xf(x) dx  \\\\\n              & = c\\text{E}[X]. \\\\\n\\end{align*} Thus we can re-write: \\begin{align*}\n\\text{E}[(X-\\mu)^2]  & = \\text{E}[ X^2 - 2X\\mu + \\mu^2] \\\\\n                        & = \\text{E}[X^2] - 2\\mu\\text{E}[X] + \\mu^2 \\\\\n                        & = \\text{E}[X^2] -2\\mu^2 + \\mu^2 \\\\\n                        & = \\text{E}[X^2] - \\mu^2 \\\\\n& = \\text{E}[X^2] - (\\text{E}[X])^2.\n\\end{align*}\n   \n\n\nQuestion 6: Functions of Random Variables\nSuppose that \\mathrm{E}[X]=\\mathrm{E}[Y]=0, \\mathrm{var}(X)=\\mathrm{var}(Y)=1, and \\mathrm{corr}(X,Y)=0.5.\n\nCompute \\mathrm{E}[3X-2Y]; and\n\\mathrm{var}(3X-2Y).\nCompute \\mathrm{E}[X^2].\n\nSolution:\n\nUsing the properties of expectations, we can re-write this as: \\begin{align*}\n\\mathrm{E}[3X-2Y] & = \\mathrm{E}[3X] + \\mathrm{E}[-2Y]\\\\\n& = 3 \\mathrm{E}[X] -2 \\mathrm{E}[Y]\\\\\n& = 3 \\times 0 -2 \\times 0\\\\\n&=0\n\\end{align*}\n\n\nUsing the properties of variances, we can re-write this as: \\begin{align*}\n\\mathrm{var}(3X-2Y) & = 3^2\\text{Var}(X) + (-2)^2\\text{Var}(Y) + 2(3)(-2)\\text{Cov}(XY)\\\\\n& =  9 \\times 1 + 4 \\times 1 -12 \\text{Corr}(XY)\\sqrt{\\text{Var}(X)\\text{Var}(Y)}\\\\\n& = 9+4 -12 \\times 0.5\\times1\\\\\n&=7\n\\end{align*}\n\n\nRecalling from Question 5 that the variance is \\mathrm{var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2, we can re-arrange to obtain: \\begin{align*}\n\\mathrm{E}[X^2] & = \\mathrm{var}(X) + (\\mathrm{E}[X])^2\\\\\n& = 1+(0)^2 \\\\\n& =1\n\\end{align*}\n\n\n\nThe Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,4) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\n\n\\displaystyle \\mathrm{Var}(\\bar{Y}) =\n\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N Y_i\\right) =\n\\frac{N}{N^2}\\mathrm{Var}(Y) =\\frac{4}{N}.\nThis is the derivation for the variance of the sampling distribution.\n \n\nWhat is the expectation of the sample mean?\n\n\\displaystyle\\mathrm{E}[\\bar{Y}] = \\frac{N}{N}\\mathrm{E}(Y) = \\mu. This is the mean of the sampling distribution.\n\n\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\n\n\\displaystyle \\mathrm{Var}(Y) = 4, because this is a sample directly from the population distribution.\n \n\nWhat is the standard error of \\bar{Y}?\n\nHere, again, we are looking at the distribution of the sample mean, so we must consider the sampling distribution, and the standard error (aka the standard distribution) is just the square root of the variance from part i.\n\\displaystyle \\mathrm{se}(\\bar{Y}) = \\sqrt{\\mathrm{Var}(\\bar{Y})} =\\frac{2}{\\sqrt{N}}.\n\n\nHypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq ` r m`, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?\n\n  \nThis question is asking you think about the hypothesis that the mean of your distribution is equal to 12. I give you the distribution of the data themselves (i.e., that they’re normal). To test the hypothesis, you work with the sampling distribution (i.e., the distribution of the sample mean) which is: \\bar{Y}\\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\nIf we knew \\sigma, we could use as our test statistic z=\\displaystyle \\frac{\\bar{y} - 12}{\\sigma/\\sqrt{n}}. However, here we need to estimate \\sigma so we use z=\\displaystyle \\frac{\\bar{y} - 12}{s_y/\\sqrt{n}} where \\displaystyle s_{y} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2}{n-1}}.\n\n\nIf the null is true, the z \\sim t_{n-1}(0,1). Since we estimate the mean frm the data, the degrees of freedom is n-1.\n\n\nAs n approaches infinity, t_{n-1}(0,1) \\rightarrow N(0,1).\n\n\nYou reject the null for \\{z: |z| &gt; t_{n-1,\\alpha/2}\\}.\n\n\nThe p-value is 2\\Pr(Z_{n-1} &gt;|z|). \n\n\nThe 95% CI is \\bar{Y} \\pm \\frac{s_{y}}{\\sqrt{n}} t_{n-1,\\alpha/2}.\n\nFor 19 out of 20 different samples, an interval constructed in this way will include the true value of the mean, \\mu. \n\nz = (11-12)/(1/3) = -3 and 2\\Pr(Z_{8} &gt;|z|) = .017, so we do reject the null.  The 95% CI for \\mu is 11 \\pm \\frac{1}{3}2.3 = (10.23, 11.77)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Disease Ecology in a Changing World: Quantitative tools to applied solutions",
    "section": "",
    "text": "The EEID work team is pleased to present the 2024 pre-conference workshop “Disease Ecology in a Changing World: Quantitative tools to applied solutions”. This webpage (and linked git repository) contains materials for the portion of the workshop “Estimating environmental response functions using Bayesian inference”. More specifically, these materials, most of which were originally developed by the VectorByte Initiative, will introduce students to simple Bayesian approaches for linking responses of organisms to an environmental driver, with most examples being drawn from the thermal trait literature.\nWe assume basic familiarity with:\n\nThe R Programming Language\nBasic calculus (especially the mathematical idea of functions)\nBasic probability and statistics (e.g., what is a probability distribution, normal and binomial distributions, means, variances)\nBasic knowledge of linear and non-linear models\nFamiliarity with the idea of a likelihood function\n\nWe’ve divided the materials into subject matter modules. Each module is designed to build on the previous one and consists of\n\nslides with presentation of materials\nlabs/hands-on materials to allow you to practice material in a practical way\nsolutions to exercises, when necessary"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the EEID 2024 Training Workshop",
    "section": "",
    "text": "This is the webpage for “Estimating environmental response functions using Bayesian inference” workshop. This workshop is part of the 2024 EEID training workshop pre-conference workshop “Disease Ecology in a Changing World: Quantitative tools to applied solutions”.\n\nMaterials for this workshop are based on those developed by members of the VectorByte Initiate and VectorBiTE RCN.\n\nCheck out our about and materials pages to continue."
  },
  {
    "objectID": "VB_Bayes_activity2.html",
    "href": "VB_Bayes_activity2.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "VB_Bayes_activity2.html#packages-and-tools",
    "href": "VB_Bayes_activity2.html#packages-and-tools",
    "title": "Introduction to Bayesian Methods",
    "section": "Packages and tools",
    "text": "Packages and tools\nFor this practical you will need to first install nimble, then be sure to install the following packages:\n\n# Load libraries\nrequire(nimble)\nrequire(HDInterval)\nlibrary(MCMCvis)\nrequire(coda) # makes diagnostic plots\nrequire(IDPmisc) # makes nice colored pairs plots to look at joint posteriors\n\n##require(mcmcplots) # another option for diagnostic plots, currently unused"
  },
  {
    "objectID": "VB_Bayes_activity2.html#example-midge-wing-length",
    "href": "VB_Bayes_activity2.html#example-midge-wing-length",
    "title": "Introduction to Bayesian Methods",
    "section": "Example: Midge Wing Length",
    "text": "Example: Midge Wing Length\nWe will use this simple example to go through the steps of assessing a Bayesian model and we’ll see that MCMC can allow us to approximate the posterior distribution.\nGrogan and Wirth (1981) provide data on the wing length (in millimeters) of nine members of a species of midge (small, two-winged flies).\nFrom these measurements we wish to make inference about the population mean \\mu.\n\n# Load data\nWL.data &lt;- read.csv(\"MidgeWingLength.csv\")\nY &lt;- WL.data$WingLength\nn &lt;- length(Y)\n\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\") \n\n\n\n\n\n\n\n\nWe’ll also need summary statistics for the data that we calculated last time:\n\nm&lt;-sum(Y)/n\ns2&lt;-sum((Y-m)^2)/(n-1)"
  },
  {
    "objectID": "VB_Bayes_activity2.html#recall-setting-up-the-bayesian-model",
    "href": "VB_Bayes_activity2.html#recall-setting-up-the-bayesian-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Recall: Setting up the Bayesian Model",
    "text": "Recall: Setting up the Bayesian Model\nWe need to define the likelihood and the priors for our Bayesian analysis. Given the analysis that we’ve just done, let’s assume that our data come from a normal distribution with unknown mean, \\mu but that we know the variance is \\sigma^2 = 0.025. That is: \n\\mathbf{Y} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\n\nIn the last activity we our prior for \\mu to be be: \n\\mu \\sim \\mathcal{N}(1.9, 0.8^2)\n Together, then, our full model is: \n\\begin{align*}\n\\mathbf{Y} & \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\\\\\n\\mu &\\sim \\mathcal{N}(1.9, 0.8^2)\n\\end{align*}\n\nIn the previous activity we wrote a function to calculate \\mu_p and \\tau_p and then plugged in our numbers:\n\ntau.post&lt;-function(tau, tau0, n){n*tau + tau0}\nmu.post&lt;-function(Ybar, mu0, sig20, sig2, n){\n  weight&lt;-sig2+n*sig20\n  \n  return(n*sig20*Ybar/weight + sig2*mu0/weight)\n}\n\nFinally we plotted 3 things together – the data histogram, the prior, and the posterior\n\nmu0 &lt;- 1.9\ns20 &lt;- 0.8\ns2&lt;- 0.025 ## \"true\" variance\n\nmp&lt;-mu.post(Ybar=m, mu0=mu0, sig20=s20, sig2=s2, n=n)\ntp&lt;-tau.post(tau=1/s2, tau0=1/s20, n=n)\n\n\nx&lt;-seq(1.3,2.3, length=1000)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8)) \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"posterior\"), col=c(2,4), lty=c(2,1), lwd=2)"
  },
  {
    "objectID": "VB_Bayes_activity2.html#specifying-the-model",
    "href": "VB_Bayes_activity2.html#specifying-the-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Specifying the model",
    "text": "Specifying the model\nFirst we must encode our choices for our data model and priors to pass them to the fitting routines in nimble. This involves setting up a {\\tt model} that includes the likelihood for each data point and a prior for every parameter we want to estimate. Here is an example of how we would do this for the simple model we fit for the midge data (note that nimble uses the precision instead of the variance or sd for the normal distribution):\n\nmodelCode &lt;-  nimbleCode({\n\n  ## Likelihood\n  for(i in 1:n){\n    Y[i] ~ dnorm(mu,tau)\n  }\n\n  ## Prior for mu\n  mu  ~ dnorm(mu0,tau0)\n\n} ## close model\n)\n\nThis model is formally in the BUGS language (also used by JAGS, WinBugs, etc). Now we will create the nimble model\n\nmodel1 &lt;- nimbleModel(code = modelCode, name = \"model1\", \n                    constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20, n=n),\n                    data  = list(Y=Y), \n                    inits = list(mu=5))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\nmodel1$getNodeNames()\n\n [1] \"mu\"   \"Y[1]\" \"Y[2]\" \"Y[3]\" \"Y[4]\" \"Y[5]\" \"Y[6]\" \"Y[7]\" \"Y[8]\" \"Y[9]\"\n\n## just checking it can compile\nCmodel1&lt;- compileNimble(model1)\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nThen we run the MCMC and, see how the output looks for a short chain:\n\nmcmc.out &lt;- nimbleMCMC(code = modelCode, \n                       constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20,n=n),\n                       data  = list(Y=Y),\n                       inits = list(mu=5),\n                       nchains = 1, niter = 100,\n                       #summary = TRUE, WAIC = TRUE,\n                       monitors = c('mu'))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ndim(mcmc.out)\n\n[1] 100   1\n\nhead(mcmc.out)\n\n           mu\n[1,] 1.819990\n[2,] 1.769434\n[3,] 1.815408\n[4,] 1.891197\n[5,] 1.841620\n[6,] 1.834539\n\n\n\nsamps&lt;-as.mcmc(mcmc.out)\nplot(samps)\n\n\n\n\n\n\n\n\nMCMC is a rejection algorithm that often needs to converge or “burn-in” – that is we need to potentially move until we’re taking draws from the correct distribution. Unlike for optimization problems, this does not mean that the algorithm heads toward a single value. Instead we’re looking for a pattern where the draws are seemingly unrelated and random. To assess convergence we look at trace plots, the goal is to get traces that look like “fuzzy caterpillars”.\nSometimes at the beginning of a run, if we start far from the area near the posterior mean of the parameter, we will instead get something that looks like a trending time series. If this is the case we have to drop the samples that were taken during the burn-in phase. Here’s an example of how to do that, also now running 2 chains simultaneously.\n\nmcmc.out &lt;- nimbleMCMC(code = modelCode, \n                       constants = list(tau=1/s2, mu0=mu0,\n                                    tau0=1/s20,n=n),\n                       data  = list(Y=Y),\n                       inits = list(mu=5),\n                       nchains = 2, niter = 11000,\n                       nburnin = 1000,\n                       #summary = TRUE, WAIC = TRUE,\n                       monitors = c('mu'))\n\nDefining model\n\n\nBuilding model\n\n\nSetting data and initial values\n\n\nRunning calculate on model\n  [Note] Any error reports that follow may simply reflect missing values in model variables.\n\n\nChecking model sizes and dimensions\n\n\nChecking model calculations\n\n\nCompiling\n  [Note] This may take a minute.\n  [Note] Use 'showCompilerOutput = TRUE' to see C++ compilation details.\n\n\nrunning chain 1...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\n\nrunning chain 2...\n\n\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\ndim(mcmc.out$chain1)\n\n[1] 10000     1\n\nhead(mcmc.out$chain1)\n\n           mu\n[1,] 1.826126\n[2,] 1.911801\n[3,] 1.788897\n[4,] 1.874767\n[5,] 1.830896\n[6,] 1.819539\n\n\n\nsamp&lt;-as.mcmc(mcmc.out$chain1)\nplot(samp)\n\n\n\n\n\n\n\n\nThis is a very fuzzy caterpillar!\nWe also often want to check the autocorrelation in the chain.\n\nacfplot(samp, lag=20, aspect=\"fill\", ylim=c(-1,1))\n\n\n\n\n\n\n\n\nThis is really good! It means that the samples are almost entirely uncorrelated.\nFinally we can also use the summary function to examine the samples generated:\n\nsummary(samp)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     1.8039244      0.0531258      0.0005313      0.0005313 \n\n2. Quantiles for each variable:\n\n 2.5%   25%   50%   75% 97.5% \n1.699 1.768 1.804 1.839 1.909 \n\n\nLet’s compare these draws to what we got with our analytic solution:\n\nx&lt;-seq(1.3,2.3, length=1000)\nhist(samp, xlab=\"mu\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8), main =\"posterior samples\") \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"analytic posterior\"), col=c(2,4), lty=c(2,1), lwd=2)\n\n\n\n\n\n\n\n\nIt worked!\nAs with the analytic approach, it’s always a good idea when you run your analyses to see how sensitive is your result to the priors you choose. Unless you are purposefully choosing an informative prior, we usually want the prior and posterior to look different, as we see here. You can experiment yourself and try changing the prior to see how this effects the posterior."
  },
  {
    "objectID": "VB_Bayes_activity2.html#practice-applying-to-a-new-dataset",
    "href": "VB_Bayes_activity2.html#practice-applying-to-a-new-dataset",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Applying to a new dataset",
    "text": "Practice: Applying to a new dataset\nDownload VecTraits dataset 562 (Kutcherov et al. 2018. Effects of temperature and photoperiod on the immature development in Cassida rubiginosa Mull. and C. stigmatica Sffr. (Coleoptera: Chrysomelidae). Sci. Rep. 9: 10047). This dataset explores the effects of temperature and photoperiod on body size (weight in mg) for Cassida stigmatica, a type of small beetle. Subset the data so you focus on one temperature/photoperiod combination (you could also choose to subset by sex). Using the same model as above, potentially with a different prior distribution, and with the value of \\tau set to 1/s^2 (where s is the empirical standard deviation of your dataset), redo the analysis above."
  },
  {
    "objectID": "VB_Bayes_activity2.html#practice-updating-the-model",
    "href": "VB_Bayes_activity2.html#practice-updating-the-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Updating the model",
    "text": "Practice: Updating the model\nRedo the previous analysis placing a gamma prior on \\mu as well. Set the prior so that the mean and variance are the same as in the normal example from above (use moment matching). Do you get something similar?"
  },
  {
    "objectID": "VB_Bayes_activity1.html",
    "href": "VB_Bayes_activity1.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "Main Materials"
  },
  {
    "objectID": "VB_Bayes_activity1.html#example-midge-wing-length",
    "href": "VB_Bayes_activity1.html#example-midge-wing-length",
    "title": "Introduction to Bayesian Methods",
    "section": "Example: Midge Wing Length",
    "text": "Example: Midge Wing Length\nWe will use this simple example to go through the steps of assessing a Bayesian model and we’ll see that MCMC can allow us to approximate the posterior distribution.\nGrogan and Wirth (1981) provide data on the wing length (in millimeters) of nine members of a species of midge (small, two-winged flies).\nFrom these measurements we wish to make inference about the population mean \\mu.\n\n# Load data\nWL.data &lt;- read.csv(\"MidgeWingLength.csv\")\nY &lt;- WL.data$WingLength\nn &lt;- length(Y)\n\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\")"
  },
  {
    "objectID": "VB_Bayes_activity1.html#non-bayesian-analysis",
    "href": "VB_Bayes_activity1.html#non-bayesian-analysis",
    "title": "Introduction to Bayesian Methods",
    "section": "Non-Bayesian analysis",
    "text": "Non-Bayesian analysis\nWe might expect that these midge data could be draws from a Normal distribution \\mathcal{N}(\\mu, \\sigma^2). Recall that the MLEs for \\mu and \\sigma^2 here are simply the sample mean and sample variance respectively:\n\nm&lt;-sum(Y)/n\ns2&lt;-sum((Y-m)^2)/(n-1)\nround(c(m, s2), 3)\n\n[1] 1.804 0.017\n\n\n\nx&lt;-seq(1.4,2.2, length=50)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.4, 2.2), freq=FALSE) \nlines(x, dnorm(x, mean=m, sd=sqrt(s2)), col=2)\n\n\n\n\n\n\n\n\nNOTE: I’ve plotted the estimate of the population distribution here, but this is not the predictive distribution (which would be a Student T because we’re estimating both the mean and variance…).\n\nThe non-Bayesian version here has the advantage of being quick and familiar. However, from our point of view it has two weaknesses:\n\nBecause we have so few data points estimates of the accuracy of our predictions aren’t available. 9 points is only barely enough to estimate a mean, so we don’t trust any of the variance calculations.\nWe can’t easily incorporate things that we might already know about midges into our analysis.\n\nLet’s see how we can do a similar analysis using a Bayesian approach, here analytically."
  },
  {
    "objectID": "VB_Bayes_activity1.html#setting-up-the-bayesian-model",
    "href": "VB_Bayes_activity1.html#setting-up-the-bayesian-model",
    "title": "Introduction to Bayesian Methods",
    "section": "Setting up the Bayesian Model",
    "text": "Setting up the Bayesian Model\nWe need to define the likelihood and the priors for our Bayesian analysis. Given the analysis that we’ve just done, let’s assume that our data come from a normal distribution with unknown mean, \\mu but that we know the variance is \\sigma^2 = 0.025. That is: \n\\mathbf{Y} \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)"
  },
  {
    "objectID": "VB_Bayes_activity1.html#prior-information",
    "href": "VB_Bayes_activity1.html#prior-information",
    "title": "Introduction to Bayesian Methods",
    "section": "Prior Information",
    "text": "Prior Information\nStudies from other populations suggest that wing lengths are usually around 1.9 mm, so we set \\mu_0 = 1.9\nWe also know that lengths must be positive (\\mu &gt;0)\nWe can approximate this restriction with a normal prior distribution for \\mu as follows:\nSince most of the normal density is within two standard deviations of the mean we choose \\tau^2_0 so that\n \\mu_0 - 2\\sigma_0 &gt;0 \\Rightarrow \\sigma_0 &lt;1.9/2 = 0.95  I will choose \\sigma_0=0.8 here. Thus our prior for mu will be: \n\\mu \\sim \\mathcal{N}(1.9, 0.8^2)\n\n\nTogether, then, our full model is: \n\\begin{align*}\n\\mathbf{Y} & \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(\\mu, 0.025^2)\\\\\n\\mu &\\sim \\mathcal{N}(1.9, 0.8^2)\n\\end{align*}"
  },
  {
    "objectID": "VB_Bayes_activity1.html#analytic-posterior",
    "href": "VB_Bayes_activity1.html#analytic-posterior",
    "title": "Introduction to Bayesian Methods",
    "section": "Analytic Posterior",
    "text": "Analytic Posterior\nFor this very simple case it is easy to write down the posterior distribution (up to some constant). First, note that the likelihood for the data can be written as\n\n\\begin{align*}\n\\mathcal{L} &\\propto \\prod_{i=1}^n \\frac{1}{\\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(Y_i-\\mu)^2 \\right) \\\\\n& =  \\frac{1}{\\sigma^n} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i-\\mu)^2 \\right)\\\\\n& \\propto \\exp\\left(-\\frac{n}{2\\sigma^2} (\\bar{Y}-\\mu)^2 \\right)\n\\end{align*}\n\nMultiplying the prior through we get the following for the posterior:\n\n\\mathrm{P}(\\mu|\\mathbf{Y}) \\propto \\exp \\left(-\\frac{n}{2\\sigma^2} (\\bar{Y}-\\mu)^2 \\right) \\exp\\left(-\\frac{1}{2\\sigma_0^2}(\\mu-\\mu_0)^2 \\right)\n\nYou can re-arrange, complete the square, etc, to get a new expression that is like\n\n\\mathrm{P}(\\mu|\\mathbf{Y}) \\propto \\exp \\left(-\\frac{1}{2\\sigma_p^2} (\\mu_p-\\mu)^2 \\right)\n\nwhere\n\n\\begin{align*}\n\\mu_p & = \\frac{n\\sigma_0^2}{\\sigma^2 + n\\sigma_0^2} \\bar{Y} +  \\frac{\\sigma^2}{\\frac{\\sigma^2}{n} + \\sigma_0^2} \\mu_0\\\\\n& \\\\\n\\sigma_p^2 & = \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1}\n\\end{align*}\n\nInstead of writing this last in terms of the variances, we could instead use precision (the inverse variance) which gives a simpler expression: \n\\tau_p = n\\tau + \\tau_0\n\nJust like in our earlier example, our estimate of the mean is a weighted average of the data and the prior, with the variance being determined by the data and prior variances.\nSo lets write a little function to calculate \\mu_p and \\tau_p and the plug in our numbers\n\ntau.post&lt;-function(tau, tau0, n){n*tau + tau0}\nmu.post&lt;-function(Ybar, mu0, sig20, sig2, n){\n  weight&lt;-sig2+n*sig20\n  \n  return(n*sig20*Ybar/weight + sig2*mu0/weight)\n}\n\nLet’s plot 3 things together – the data histogram, the prior, and the posterior\n\nmu0 &lt;- 1.9\ns20 &lt;- 0.8\ns2&lt;- 0.025 ## \"true\" variance\n\nmp&lt;-mu.post(Ybar=m, mu0=mu0, sig20=s20, sig2=s2, n=n)\ntp&lt;-tau.post(tau=1/s2, tau0=1/s20, n=n)\n\n\nx&lt;-seq(1.3,2.3, length=1000)\nhist(Y,breaks=10,xlab=\"Wing Length (mm)\", xlim=c(1.3, 2.3),\n     freq=FALSE, ylim=c(0,8)) \nlines(x, dnorm(x, mean=mu0, sd=sqrt(s20)), col=2, lty=2, lwd=2) ## prior\nlines(x, dnorm(x, mean=mp, sd=sqrt(1/tp)), col=4, lwd=2) ## posterior\nlegend(\"topleft\", legend=c(\"prior\", \"posterior\"), col=c(2,4), lty=c(2,1), lwd=2)"
  },
  {
    "objectID": "VB_Bayes_activity1.html#practice-prior-sensitivity",
    "href": "VB_Bayes_activity1.html#practice-prior-sensitivity",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice: Prior sensitivity",
    "text": "Practice: Prior sensitivity\nChange the values of the mean and the variance that you choose for the prior (“hyperparameters”). What does this do to the posterior distribution. E.g., what happens if the variance you choose is small, and \\mu_0 =2.5 or so. Is this what you expect?"
  },
  {
    "objectID": "EEID_Tools.html#learning-objectives",
    "href": "EEID_Tools.html#learning-objectives",
    "title": "EEID 2024 Quantitative Training",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nWe will give you a brief overview to the tools that we will use, and how to access trait data either through the web or using one of the tools.\nYou will then complete a practical to give you a chance to install tools and practice an introductory task."
  },
  {
    "objectID": "EEID_Tools.html#tools-for-bayesian-analysis",
    "href": "EEID_Tools.html#tools-for-bayesian-analysis",
    "title": "EEID 2024 Quantitative Training",
    "section": "Tools for Bayesian Analysis",
    "text": "Tools for Bayesian Analysis\nAlthough it is entirely possible to program a Bayesian analysis from scratch in your favorite programming language, most folks use tools designed for this purpose.\nFor example, one of the earliest was BUGS/WinBUGS, and other software, such as JAGS/rjags grew out of it."
  },
  {
    "objectID": "EEID_Tools.html#installing-nimble-and-bayestpc",
    "href": "EEID_Tools.html#installing-nimble-and-bayestpc",
    "title": "EEID 2024 Quantitative Training",
    "section": "Installing nimble and bayesTPC",
    "text": "Installing nimble and bayesTPC\nInstalling both packages will proceed in three steps (more details in the practical)\n\nensure that you have appropriate compilers installed, and the R developer tools (devtools package)\ninstall nimble from CRAN\ninstall bayesTPC from GitHub\n\nYou will need to install these in this order to ensure that everything will work properly."
  },
  {
    "objectID": "EEID_Tools.html#accessing-trait-data",
    "href": "EEID_Tools.html#accessing-trait-data",
    "title": "EEID 2024 Quantitative Training",
    "section": "Accessing Trait Data",
    "text": "Accessing Trait Data\nIf you are doing experimental work, then maybe you are generating your own trait data.\n\nHowever, often we want or need to find published trait data that we can use to parameterize our models."
  },
  {
    "objectID": "EEID_Tools.html#vectorbytes-vectraits-database",
    "href": "EEID_Tools.html#vectorbytes-vectraits-database",
    "title": "EEID 2024 Quantitative Training",
    "section": "VectorByte’s VecTraits Database",
    "text": "VectorByte’s VecTraits Database\nWe will primarily use data from VecTraits for three reasons:\n\nthe database is specific to organisms involved in disease transmission\nthe database is designed on FAIR principles (with Interoperability being the key one here)\naccess to VecTraits is incorporated into bayesTPC making it straightforward to include it into an analysis workflow\n\nWe can explore data through the VecTraits Online Interface."
  },
  {
    "objectID": "EEID_Tools.html#practical",
    "href": "EEID_Tools.html#practical",
    "title": "EEID 2024 Quantitative Training",
    "section": "Practical",
    "text": "Practical\nYou will now move on to the hands-on portion of this section. You will:\n\nget tools installed\nchoose a dataset you are interested in playing with through the VecTraits web interface\npractice downloading your chosen dataset using bayesTPC\nlearn a bit about the downloaded format so you can choose the appropriate columns and visualize your chosen dataset."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "",
    "text": "Most of the materials here were initially developed as part of the VectorByte Initiative and the older VectorBiTE RCN. As such, they have been developed with the effort of many people over the years. Most of the specific materials for this workshop were originally developed by Dr. Leah R. Johnson and Sean Sorek. They have been modified for this workshop by Leah Johnson and Victor Pena."
  },
  {
    "objectID": "materials.html#hardware-and-software",
    "href": "materials.html#hardware-and-software",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Hardware and Software",
    "text": "Hardware and Software\nWe will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 4.2 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop.\nWe will also be using a new package, bayesTPC. We suggest that you install this package in advance. Note that the nimble package must be installed and loaded before bayesTPC can be used. To install bayesTPC you can use the following code:\n\nremotes::install_github(\"johnwilliamsmithjr/bayesTPC\")"
  },
  {
    "objectID": "materials.html#pre-requisites",
    "href": "materials.html#pre-requisites",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nWe are assuming familiarity with R basics as well as at least introductory statistics, including up through linear models and the idea of a likelihood. If you would like materials to review, we recommend that you do the following:\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. But there are many more resources online (e.g., this and this ) – pick something that suits your learning style.\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics"
  },
  {
    "objectID": "materials.html#introduction-to-traits-in-disease-modeling",
    "href": "materials.html#introduction-to-traits-in-disease-modeling",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Introduction to traits in Disease modeling",
    "text": "Introduction to traits in Disease modeling\n\nLecture slides: Coming Soon\nCator et al. 2020. The Role of Vector Trait Variation in Vector-Borne Disease Dynamics"
  },
  {
    "objectID": "materials.html#intro-to-course-tools",
    "href": "materials.html#intro-to-course-tools",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Intro to Course Tools",
    "text": "Intro to Course Tools\n\nLecture Slides, Practical: Coming Soon"
  },
  {
    "objectID": "materials.html#intro-to-bayes",
    "href": "materials.html#intro-to-bayes",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Intro to Bayes",
    "text": "Intro to Bayes\n\nLecture Slides, Practical 1\nDatasets:\n\nMidge data"
  },
  {
    "objectID": "materials.html#bayesian-computation-and-mcmc",
    "href": "materials.html#bayesian-computation-and-mcmc",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Bayesian computation and MCMC",
    "text": "Bayesian computation and MCMC\n\nLecture Slides, Practical 2A"
  },
  {
    "objectID": "materials.html#fitting-tpcs-using-bayestpc",
    "href": "materials.html#fitting-tpcs-using-bayestpc",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Fitting TPCs using bayesTPC",
    "text": "Fitting TPCs using bayesTPC\n\nPractical"
  },
  {
    "objectID": "materials.html#advanced-features-in-bayestpc",
    "href": "materials.html#advanced-features-in-bayestpc",
    "title": "EEID 2024 Workshop Training Materials",
    "section": "Advanced features in bayesTPC",
    "text": "Advanced features in bayesTPC\n\nPractical: Coming Soon"
  },
  {
    "objectID": "bayesTPC_advanced.html",
    "href": "bayesTPC_advanced.html",
    "title": "Advanced topics with bayesTPC",
    "section": "",
    "text": "In this section, we will see how to use a custom functional form for thermal performance curves that is not currently implemented in `bayesTPC`. We will also see how to process the output from bayesTPC to generate custom plots or get the posterior distribution of model parameters."
  },
  {
    "objectID": "bayesTPC_advanced.html#thermal-performance-curves-under-antibiotics",
    "href": "bayesTPC_advanced.html#thermal-performance-curves-under-antibiotics",
    "title": "Advanced topics with bayesTPC",
    "section": "Thermal performance curves under antibiotics",
    "text": "Thermal performance curves under antibiotics\nLet’s take a look at a dataset of the temperature-dependence of the growth of the bacterium Escherichia coli in the presence of various antibiotic backgrounds (data originally from this study).\n\nabdata &lt;- read.csv(\"ab_data.csv\")\nhead(abdata)\n\n  drug1name drug2name drug1num drug2num  T  t sample          OD\n1       ERY       GEN        4        6 37 24      1 0.368300008\n2       ERY       GEN        4        6 37 24      2 0.404299991\n3       ERY       GEN        4        6 37 24      3 0.423800008\n4       ERY       GEN        4        6 37 24      4 0.427200006\n5       ERY       GEN        4        6 22 24      1 0.004800001\n6       ERY       GEN        4        6 22 24      2 0.005099999\n\n\nThis dataset consists of optical density (OD) values (which are proportional to the number of bacteria) of E. coli cultures after 24 hour growth at various temperatures (T) under fixed concentrations of 12 antibiotics and all their pairwise combinations. There are four replicates per drug/temperature treatment.\nIn this workshop we will focus on comparing how the thermal performance for E. coli growth looks like under four conditions of interest:\n\nno antibiotic\ngentamicin (GEN)\nerythromycin (ERY)\nGEN+ERY (both antibiotics present at the same time)\n\n\n# The data uses \"WT\" to encode \"no drug\".\nnodrug &lt;- subset(abdata, (abdata[\"drug1name\"] == \"WT\") & (abdata[\"drug2name\"] == \"WT\"))\nGEN &lt;- subset(abdata, (abdata[\"drug1name\"] == \"GEN\") & (abdata[\"drug2name\"] == \"WT\"))\nERY &lt;- subset(abdata, (abdata[\"drug1name\"] == \"ERY\") & (abdata[\"drug2name\"] == \"WT\"))\nboth &lt;- subset(abdata, (abdata[\"drug1name\"] == \"ERY\") & (abdata[\"drug2name\"] == \"GEN\"))\n\nLet’s start by plotting the data.\n\npar(mfrow=c(2,2))\nplot(nodrug$T, nodrug$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"No drug\", ylim=c(0, 1))\nplot(GEN$T, GEN$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"GEN\", ylim=c(0, 1))\nplot(ERY$T, ERY$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"ERY\", ylim=c(0, 1))\nplot(both$T, both$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"GEN+ERY\", ylim=c(0, 1))\n\n\n\n\n\n\n\n\nAs might be expected, adding antibiotics to the growth media decreases the number of bacteria. However, they can decrease growth more at some temperatures than others, leading to changes in the shape of the TPC.\nSome of the shapes you get when adding antibiotics do not look like typical TPCs and it could be difficult to fit this data with many common TPC models. In the following section, we show how we can add a new functional form to bayesTPC that can describe these TPCs."
  },
  {
    "objectID": "bayesTPC_advanced.html#a-flexible-and-interpretable-model-for-thermal-performance-curves",
    "href": "bayesTPC_advanced.html#a-flexible-and-interpretable-model-for-thermal-performance-curves",
    "title": "Advanced topics with bayesTPC",
    "section": "A flexible and interpretable model for thermal performance curves",
    "text": "A flexible and interpretable model for thermal performance curves\nNow we introduce a new functional form for thermal performance curves called flexTPC. This model aims to be both flexible (aiming to describe unimodal TPC curves of any skewness) and interpretable (with all model parameters having a clear biological interpretation).\nA preprint focusing on describing this model and showing its performance in various different datasets is in preparation and will be published soon. In the meantime, you can check the associated GitHub.\nThe equation for the flexTPC model is\n\\[\nr(T) = r_{\\max}\\left[\\left(\\frac{T - T_{\\min}}{\\alpha} \\right)^{\\alpha} \\left(\\frac{T_{\\max} - T}{1 - \\alpha} \\right)^{1-\\alpha}\n\\left(\\frac{1}{T_{\\max} - T_{\\min}} \\right)\n\\right]^\\frac{\\alpha (1 - \\alpha)}{\\beta^2}\n\\]\nwhere\n\n\\(T_{\\min}\\) is the minimum temperature,\n\\(T_{\\max}\\) the maximum temperature,\n\\(r_{\\max}\\) the maximum trait value/performance of the TPC,\n\\(\\alpha \\in [0,1]\\) determines where the optimal temperature \\(T_{\\mathrm{opt}}\\) is relative to \\(T_{\\min}\\) and \\(T_{\\max}\\) through the equation \\[\nT_{\\mathrm{opt}} = \\alpha T_{\\max} + (1 - \\alpha) T_{\\min}\n\\] (where, for example, \\(\\alpha = 0\\) corresponds to \\(T_{\\mathrm{opt}} = T_{\\min}\\), \\(\\alpha = 1\\) corresponds to \\(T_{\\mathrm{opt}} = T_{\\max}\\) and \\(\\alpha = 1/2\\) corresponds to a symmetric TPC where \\(T_{\\mathrm{opt}} = (T_{\\min} + T_{\\max}) / 2\\)), and\n\\(\\beta &gt; 0\\) determines the breadth of the TPC near its peak.\n\nIt may be more intuitive to look at how the predicted TPC changes as we modify each of these parameters. You can change each parameter in flexTPC in the visualization below to see how it affects the shape of the curve. As you can see, flexTPC can describe curves of a wide variety of shapes, as long as they are unimodal (that is, have a single peak).\n\nviewof T_min = Inputs.range(\n  [-5, 20], \n  {value: 10, step: 0.2, label: \"Tmin:\"}\n)\nviewof T_max = Inputs.range(\n  [30, 50], \n  {value: 35, step: 0.2, label: \"Tmax:\"}\n)\n\nviewof r_max = Inputs.range(\n  [0, 1.2], \n  {value: 1, step: 0.1, label: \"rmax:\"}\n)\n\nviewof alpha = Inputs.range(\n  [0, 1], \n  {value: 0.8, step: 0.02, label: \"alpha:\"}\n)\n\nviewof beta = Inputs.range(\n  [0, 1], \n  {value: 0.3, step: 0.02, label: \"beta:\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction incrementalArray(start, end, step) {\n    var arr = [];\n    // convert count to an integer to avoid rounding errors\n    var count = +((end - start) / step).toFixed();\n    for (var j = 0; j &lt;= count; j++) {\n        var i = start + j * step;\n        arr.push(i);\n    }\n    return arr;\n}\n\nfunction flexTPC(x, T_min, T_max, r_max, alpha, beta) {\n  if (x &lt; T_min) {\n  return 0.0\n  }\n  if (x &gt; T_max) {\n  return 0.0\n  }\n  return r_max * Math.exp((alpha * (1.0 - alpha) / beta**2) * (alpha *    Math.log( (x - T_min) / alpha)  + \n      (1.0 - alpha) * Math.log( (T_max - x) / (1.0 - alpha)) -\n      Math.log(T_max - T_min)))\n}\n\nxvals = incrementalArray(-5, 50, 0.1)\nyvals = xvals.map((x) =&gt; flexTPC(x, T_min, T_max, r_max, alpha, beta))\nzip = (a, b) =&gt; a.map((k, i) =&gt; [k, b[i]]);\ndata = zip(xvals, yvals)\n\nPlot.plot({\n  width: 600,\n  height: 400,\n  y: { domain: [-0.01, 1.25] },\n  marks: [\n    Plot.line(\n      data,\n      {\n        strokeWidth: 3,\n        stroke: \"steelblue\",\n        fontSize: 14\n      }\n    ),\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.axisX({label: \"Temperature [°C]\", fontSize: 14, marginBottom: 40}),\n    Plot.axisY({label: \"trait performance\", fontSize: 14, x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(there’s supposed to be an interactive visualization above this line, hopefully this works on the website!).\n\nAdding the flexTPC function to bayesTPC\nWe want to add the flexTPC functional form to bayesTPC. To do this, we need two things:\n\nWe need to define an expression with the formula we want to use.\nWe need to define default prior distributions for every parameter in the model. As we will be focusing on the antibiotic dataset presented earlier, we will define the priors we want to use in this case here directly rather than code in default non-restrictive priors.\n\n\n## Note: This looks a little different from the equation as written above, but is\n## equivalent to it. It's just written a little differently for numerical stability.\nflexTPC_formula &lt;- expression((T_max &gt; Temp) * (T_min &lt; Temp) * r_max * exp((alpha * (1 - alpha) / beta^2) * (alpha * log( max((Temp - T_min) / alpha, 10^-20)) \n                                                                                    + (1 - alpha) * log(max ((T_max - Temp) / (1 - alpha), 10^-20))\n                                                                                    - log(T_max - T_min)) ) )\n\n# Prior distributions.\nflexTPC_priors &lt;- c(\n  r_max = \"dunif(0, 1)\", # Maximum trait value. Chosen because OD values are less than one and because we want to give equal prior probability to all values in this interval.\n  T_max = \"dnorm(46, 1 / 2^2)\", ## Normal prior with mu=46, sigma=2. Assumes 95% prior CI of approximately [42°C, 50°C].\n  T_min = \"dnorm(10, 1 / 5^2)\", ## Normal prior with mu=10, sigma=5. Assumes 95% prior CI of approximately [0°C, 20°C]\n  alpha = \"dunif(0, 1)\", ## Uniform prior in alpha places equal prior probability on T_opt being anywhere in-between T_min and T_max.\n  beta = \"dgamma(0.3^2 / 0.2^2, 0.3 / 0.2^2)\") ## Gamma prior with mean of 0.3 and standard deviation of 0.2. Asssumes 95% prior CI of approx [0.01, 0.99] for beta. Typical TPCs like those that are described by the Briere and quadratic models have values around 0.2-0.4.\n\nflexTPC_normal &lt;- specify_normal_model(\"flexTPC_normal\", #model name\n                                  parameters = flexTPC_priors, #names are parameters, values are priors\n                                  formula = flexTPC_formula\n)\n\nModel type 'flexTPC_normal' can now be accessed using other bayesTPC functions. Restart R to reset back to defaults.\n\nget_formula(\"flexTPC_normal\")\n\nexpression((T_max &gt; Temp) * (T_min &lt; Temp) * r_max * exp((alpha * \n    (1 - alpha)/beta^2) * (alpha * log(max((Temp - T_min)/alpha, \n    10^-20)) + (1 - alpha) * log(max((T_max - Temp)/(1 - alpha), \n    10^-20)) - log(T_max - T_min))))\n\nget_default_priors(\"flexTPC_normal\")\n\n                               r_max                                T_max \n                       \"dunif(0, 1)\"                 \"dnorm(46, 1 / 2^2)\" \n                               T_min                                alpha \n                \"dnorm(10, 1 / 5^2)\"                        \"dunif(0, 1)\" \n                                beta \n\"dgamma(0.3^2 / 0.2^2, 0.3 / 0.2^2)\" \n\ncat(configure_model(flexTPC_normal))\n\n{\n    for (i in 1:N){\n        m[i] &lt;- ( (T_max &gt; Temp[i]) * (T_min &lt; Temp[i]) * r_max * exp((alpha * (1 - alpha)/beta^2) * (alpha * log(max((Temp[i] - T_min)/alpha, 10^-20)) + (1 - alpha) * log(max((T_max - Temp[i])/(1 - alpha), 10^-20)) - log(T_max - T_min))) )\n        Trait[i] ~ T(dnorm(mean = m[i], tau = 1/sigma.sq), 0, )\n    }\n    r_max ~ dunif(0, 1)\n    T_max ~ dnorm(46, 1 / 2^2)\n    T_min ~ dnorm(10, 1 / 5^2)\n    alpha ~ dunif(0, 1)\n    beta ~ dgamma(0.3^2 / 0.2^2, 0.3 / 0.2^2)\n    sigma.sq ~ dexp(1)\n}\n\n\nNow let’s get the data in the right shape and fit the flexTPC model to all these conditions.\n\nnodrug.data.bTPC&lt;-list(Trait = nodrug$OD, Temp=nodrug$T)\nnodrugFit &lt;- b_TPC(data = nodrug.data.bTPC, ## data\n                     model = 'flexTPC_normal', ## model to fit\n                     niter = 50000, ## total iterations\n                     burn = 10000, ## number of burn in samples\n                     samplerType = 'AF_slice', ## slice sampler\n                     priors = list(sigma.sq = 'dexp(1 / 0.1^2)'), ## priors\n                     thin = 2,\n                     inits = list(\"T_min\"=15, \"T_max\"=46, \"alpha\"=0.8,\n                                  \"beta\"=0.3, \"r_max\"=0.8)\n) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\nGEN.data.bTPC&lt;-list(Trait = GEN$OD, Temp=GEN$T)\nGENFit &lt;- b_TPC(data = GEN.data.bTPC, ## data\n                     model = 'flexTPC_normal', ## model to fit\n                     niter = 50000, ## total iterations\n                     burn = 10000, ## number of burn in samples\n                     samplerType = 'AF_slice', ## slice sampler\n                     priors = list(sigma.sq = 'dexp(1 / 0.1^2)'), ## priors\n                     thin = 2,\n                     inits = list(\"T_min\"=15, \"T_max\"=46, \"alpha\"=0.5,\n                                  \"beta\"=0.7, \"r_max\"=0.5)\n) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\nERY.data.bTPC&lt;-list(Trait = ERY$OD, Temp=ERY$T)\nERYFit &lt;- b_TPC(data = ERY.data.bTPC, ## data\n                     model = 'flexTPC_normal', ## model to fit\n                     niter = 50000, ## total iterations\n                     burn = 10000, ## number of burn in samples\n                     samplerType = 'AF_slice', ## slice sampler\n                     priors = list(sigma.sq = 'dexp(1 / 0.1^2)'), ## priors\n                     thin = 2,\n                     inits = list(\"T_min\"=15, \"T_max\"=46, \"alpha\"=0.8,\n                                  \"beta\"=0.3, \"r_max\"=0.8)\n) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\nboth.data.bTPC&lt;-list(Trait = both$OD, Temp=both$T)\nbothFit &lt;- b_TPC(data = both.data.bTPC, ## data\n                     model = 'flexTPC_normal', ## model to fit\n                     niter = 50000, ## total iterations\n                     burn = 10000, ## number of burn in samples\n                     samplerType = 'AF_slice', ## slice sampler\n                     priors = list(sigma.sq = 'dexp(1 / 0.1^2)'), ## priors\n                     thin = 2,\n                     inits = list(\"T_min\"=15, \"T_max\"=46, \"alpha\"=0.8,\n                                  \"beta\"=0.3, \"r_max\"=0.8)\n) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\n\nMost of the traceplots look OK, but here is an example of one that doesn’t look great:\n\npar(mfrow=c(3, 2))\ntraceplot(ERYFit, burn=10000)\n\n\n\n\n\n\n\n\nWe can see than not all of our samples look like a “hairy caterpillar”. As can be seen in the traceplot for \\(r_{\\max}\\), occassionally the samples seem to “get stuck” in some regions of the parameter space. This means the MCMC chains may be converging slowly for some parameters.\nA few other things can help with convergence of the MCMC chains:\n\nTry another sampler.\nTry setting the initial values of the parameters to values that are likely to be near the best fitting values.\nUse stronger prior distributions.\nRun the chains for longer.\n\nThere are also some diagnostics based on running multiple chains (ideally started from different initial values) that can help us evaluate convergence based on how similar the samples from the different chains are to each other. Future versions of flexTPC will include the ability to run multiple chains and some of these diagnostics.\nIn this tutorial we’re running the chains for 50000 iterations (instead of 10000 as in previous examples) and providing reasonable initial values. For final results, we’d likely run the chains even longer to be safe, although it wouldn’t be practical to do this here due to time constraints.\nHowever, most of the parameters show good mixing and the TPCs we get look reasonable when plotted with the data (see below). So it’s likely OK to use our samples for the analysis."
  },
  {
    "objectID": "bayesTPC_advanced.html#advanced-topic-transformations-of-model-parameters",
    "href": "bayesTPC_advanced.html#advanced-topic-transformations-of-model-parameters",
    "title": "Advanced topics with bayesTPC",
    "section": "Advanced topic: Transformations of model parameters",
    "text": "Advanced topic: Transformations of model parameters\nThe logical next step before proceeding with the analysis is to plot the curves along with the data. However, we will do this a little bit later in this practical, since we want to show how to do this directly from the MCMC samples to see how we can customize our plots.\nWhen using MCMC methods, we obtain samples from the posterior distribution of the parameters in our model. Let’s take a look at the first few samples from the condition with no antibiotics.\n\nhead(nodrugFit$samples)\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n        T_max    T_min     alpha      beta     r_max    sigma.sq\n[1,] 44.00235 17.85471 0.8784877 0.3704695 0.7989890 0.001292913\n[2,] 44.00206 17.72974 0.8779760 0.3629857 0.8084367 0.001044803\n[3,] 44.00816 17.53666 0.8631058 0.3718653 0.7954786 0.001699794\n[4,] 44.07525 16.31403 0.8341328 0.3471373 0.7968837 0.001915781\n[5,] 44.07119 18.36344 0.8046585 0.3917791 0.8207441 0.002398045\n[6,] 44.06693 18.64919 0.8268135 0.4072552 0.7717190 0.001343461\n[7,] 44.01967 18.69668 0.8363670 0.4069947 0.7929890 0.001128260\n\n\nEach row corresponds to the values of the parameters in one iteration of the chain. Once the MCMC chain has reached convergence, each iteration corresponds to drawing a sample from the posterior distribution.\nWe can plot the posterior distribution for the individual parameters using these samples. For example, we may want to compare the height of the TPCs (which corresponds to parameter $r_{}$) between the different antibiotic backgrounds. In this data, this would correspond to the maximum number of bacteria that grow under the corresponding antibiotic condition at any temperature.\n\npar(mfrow=c(2, 2))\nhist(nodrugFit$samples[, \"r_max\"], main=\"No drug\", xlab=\"r_max\", xlim=c(0,1))\nhist(ERYFit$samples[, \"r_max\"], main=\"ERY\", xlab=\"r_max\", xlim=c(0,1))\nhist(GENFit$samples[, \"r_max\"], main=\"GEN\", xlab=\"r_max\", xlim=c(0,1))\nhist(bothFit$samples[, \"r_max\"], main=\"ERY+GEN\", xlab=\"r_max\", xlim=c(0,1))\n\n\n\n\n\n\n\n\nWe can also use the MCMC samples to obtain the posterior distribution of any function involving the model parameters. For example, we might be interested in the difference between the maximum growth observed under the conditions with antibiotics present and when there is no antibiotics (note: to do this we need to have the same number of samples for all chains we are comparing).\n\npar(mfrow=c(2, 2))\nhist(ERYFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], main=\"ERY\", xlab=\"r_max difference\", xlim=c(-1,1))\nhist(GENFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], main=\"GEN\", xlab=\"r_max difference\", xlim=c(-1,1))\nhist( bothFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], main=\"ERY+GEN\", xlab=\"r_max difference\", xlim=c(-1,1))\n\n\n\n\n\n\n\n\nLet’s calculate medians and credible intervals for this differences.\n\nprint(\"ERY\")\n\n[1] \"ERY\"\n\nquantile(ERYFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], c(0.025, 0.5, 0.975))\n\n      2.5%        50%      97.5% \n-0.3206317 -0.1540494  0.1520965 \n\nprint(\"GEN\")\n\n[1] \"GEN\"\n\nquantile(GENFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], c(0.025, 0.5, 0.975))\n\n      2.5%        50%      97.5% \n-0.4952626 -0.4459120 -0.4003555 \n\nprint(\"ERY+GEN\")\n\n[1] \"ERY+GEN\"\n\nquantile(bothFit$samples[, \"r_max\"] - nodrugFit$samples[, \"r_max\"], c(0.025, 0.5, 0.975))\n\n      2.5%        50%      97.5% \n-0.3556227 -0.2745906  0.1324121 \n\n\nIn the flexTPC model we have an explicit equation for the optimal temperature\n\\[ T_{\\mathrm{opt}}= \\alpha T_{\\max} + (1 - \\alpha) T_{\\min} \\]\nUsing this equation, we can also get posterior samples for \\(T_{\\mathrm{opt}}\\) by transforming the posterior samples for \\(T_{\\min}\\), \\(T_{\\max}\\) and \\(\\alpha\\). For example, for the no-antibiotics condition,\n\n# Function to calculate Topt in flexTPC model from Tmin, Tmax and alpha.\nT_opt_fn &lt;- function(Tmin, Tmax, alpha) {\n  return(alpha * Tmax + (1 - alpha) * Tmin)\n}\npar(mfrow=c(1, 1))\nT_opt_samples &lt;- apply(nodrugFit$samples, 1, function(x) T_opt_fn(x[['T_min']], x[['T_max']], x[['alpha']]))\nhist(T_opt_samples)\n\n\n\n\n\n\n\n\nWe can then obtain summaries of the posterior distribution of \\(T_{\\mathrm{opt}}\\) such as the mean, median and/or credible intervals.\n\nmean(T_opt_samples)\n\n[1] 39.13421\n\nquantile(T_opt_samples, c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n37.90336 39.10419 40.50628 \n\n\nWe can follow a similar approach to obtain the posterior distribution of the value of the curve at any temperature (or a grid of temperatures). In this case, the transformation of the parameters we want is simply the equation for the flexTPC model itself to get the posterior distribution of the TPC at temperature \\(T\\). We can then directly calculate medians and credible intervals to plot the TPCs.\nIt can be useful to calculate these values directly if we want to customize our plots to make them nicer compared to the bayesTPC defaults:\n\n## Grid of temperatures to use to plot the TPCs.\ntemp &lt;- seq(11, 49, 0.1)\n\n# FlexTPC equation\nflexTPC &lt;- function(T, Tmin, Tmax, rmax, alpha, beta) {\n  s &lt;- alpha * (1 - alpha) / beta^2\n  Tidx &lt;- (T &gt; Tmin) & (T &lt; Tmax)\n  result &lt;- rep(0, length(T))\n  result[Tidx] &lt;- rmax * exp(s *(alpha * log((T[Tidx] - Tmin) / alpha ) +\n                             (1 - alpha) * log((Tmax - T[Tidx]) / ( 1 - alpha) )\n                           - log(Tmax - Tmin)))\n  return(result)\n}\n\npar(mfrow=c(2,2))\nplot(nodrug$T, nodrug$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"No drug\", ylim=c(0, 1), xlim=c(10, 50))\nndcurves &lt;- apply(nodrugFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(ndcurves, 1, median), col='black', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(ndcurves, 1, quantile, 0.025),\n                                rev(apply(ndcurves, 1, quantile, 0.975))), \n        col=alpha(\"black\", 0.3), lty=0)\n\nplot(GEN$T, GEN$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"GEN\", ylim=c(0, 1), xlim=c(10, 50))\nGENcurves &lt;- apply(GENFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(GENcurves, 1, median), col='steelblue', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(GENcurves, 1, quantile, 0.025),\n                                rev(apply(GENcurves, 1, quantile, 0.975))), \n        col=alpha(\"steelblue\", 0.3), lty=0)\n\nplot(ERY$T, ERY$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"ERY\", ylim=c(0, 1), xlim=c(10, 50))\nERYcurves &lt;- apply(ERYFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(ERYcurves, 1, median), col='darkgreen', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(ERYcurves, 1, quantile, 0.025),\n                                rev(apply(ERYcurves, 1, quantile, 0.975))), \n        col=alpha(\"darkgreen\", 0.3), lty=0)\n\nplot(both$T, both$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"GEN+ERY\", ylim=c(0, 1), xlim=c(10, 50))\nbothcurves &lt;- apply(bothFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(bothcurves, 1, median), col='purple', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(bothcurves, 1, quantile, 0.025),\n                                rev(apply(bothcurves, 1, quantile, 0.975))), \n        col=alpha(\"purple\", 0.3), lty=0)\n\n\n\n\n\n\n\n\nWe can also plot all the TPCs together to more easily compare them.\n\npar(mfrow=c(1,1))\ntemp &lt;- seq(10, 50, 0.1)\nplot(\"\", \"\", xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"All TPCs\", ylim=c(0, 1), xlim=c(10, 50))\nndcurves &lt;- apply(nodrugFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(ndcurves, 1, median), col='black', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(ndcurves, 1, quantile, 0.025),\n                                rev(apply(ndcurves, 1, quantile, 0.975))), \n        col=alpha(\"black\", 0.3), lty=0)\n\nGENcurves &lt;- apply(GENFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(GENcurves, 1, median), col='steelblue', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(GENcurves, 1, quantile, 0.025),\n                                rev(apply(GENcurves, 1, quantile, 0.975))), \n        col=alpha(\"steelblue\", 0.3), lty=0)\n\nERYcurves &lt;- apply(ERYFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(ERYcurves, 1, median), col='darkgreen', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(ERYcurves, 1, quantile, 0.025),\n                                rev(apply(ERYcurves, 1, quantile, 0.975))), \n        col=alpha(\"darkgreen\", 0.3), lty=0)\n\nbothcurves &lt;- apply(bothFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(bothcurves, 1, median), col='purple', lwd=2)\npolygon(c(temp, rev(temp)), c(apply(bothcurves, 1, quantile, 0.025),\n                                rev(apply(bothcurves, 1, quantile, 0.975))), \n        col=alpha(\"purple\", 0.3), lty=0)\n\n\n\n\n\n\n\n\nHere, the curve with no antibiotics is shown in black. We can see that the GEN+ERY (purple) curve looks very similar to the curve with only ERY (green). In both cases, the number of bacteria is reduced more sharply at low temperatures. The TPC for GEN (blue) looks very different, reducing the number of bacteria more sharply at high temperatures."
  },
  {
    "objectID": "bayesTPC_advanced.html#model-selection",
    "href": "bayesTPC_advanced.html#model-selection",
    "title": "Advanced topics with bayesTPC",
    "section": "Model selection",
    "text": "Model selection\nIn this section, we show how we can compare between different models with bayesTPC using WAIC. We will compare two candidate models:\na) A single TPC model for all of the antibiotic conditions. This corresponds to a null hypothesis of antibiotics not affecting the thermal performance curve.\nb) Separate TPC models for each antibiotic background, as we had before.\n\n# Make a single dataset with all data.\nallconds &lt;- bind_rows(nodrug, GEN, ERY, both)\nallconds.data.bTPC&lt;-list(Trait = allconds$OD, Temp=allconds$T)\nallcondsFit &lt;- b_TPC(data = allconds.data.bTPC, ## data\n                     model = 'flexTPC_normal', ## model to fit\n                     niter = 50000, ## total iterations\n                     burn = 10000, ## number of burn in samples\n                     samplerType = 'AF_slice', ## slice sampler\n                     priors = list(sigma.sq = 'dexp(1 / 0.1^2)'), ## priors\n                     thin=2\n) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\n\n\nplot(allconds$T, allconds$OD, xlab=\"Temperature [°C]\", ylab=\"OD\", main=\"All data together\", ylim=c(0, 1), xlim=c(10, 50))\naccurves &lt;- apply(allcondsFit$samples, 1, function(x) flexTPC(temp, x[['T_min']], x[['T_max']],\n                                                          x[['r_max']], x[['alpha']], x[['beta']]))\nlines(temp, apply(accurves, 1, median), col='black', lwd=2)\n\npolygon(c(temp, rev(temp)), c(apply(accurves, 1, quantile, 0.025),\n                                rev(apply(accurves, 1, quantile, 0.975))), \n        col=alpha(\"black\", 0.3), lty=0)\n\n\n\n\n\n\n\n\nNow we can do model selection through WAIC. First, let’s find the WAIC for the single curve.\n\nallcondsFit$mcmc$getWAIC()$WAIC\n\n[1] -120.1075\n\n\nWe can now find the WAICs from the individual curves. WAIC is additive, so we can simply add the WAICs for the individual curves to get a value that we can compare to that calculated above.\n\n(nodrugFit$mcmc$getWAIC()$WAIC + ERYFit$mcmc$getWAIC()$WAIC + GENFit$mcmc$getWAIC()$WAIC + bothFit$mcmc$getWAIC()$WAIC)\n\n  [Warning] There are 3 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n  [Warning] There are 2 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n  [Warning] There are 7 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n  [Warning] There are 2 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n\n\n[1] -395.3301\n\n\nThe WAIC for the individual curves is much lower (more negative) than for the single curve describing all data. Based on this, modeling this data as individual TPCs is preferred compared to having a single curve for all conditions.\nHowever, we get some warnings about the WAIC estimates for the individual curves potentially being unstable. These are likely due to the smaller sample size, which makes some individual data points very influential in the WAIC calculation.\nIn this case, the difference between the WAICs is so large that this potential inaccuracy is not likely to lead to choosing the wrong model. However, to be safe, the WAIC could be compared to other model selection criteria like cross validation (currently not implemented in bayesTPC)."
  },
  {
    "objectID": "VB_Bayes2.html#learning-objectives",
    "href": "VB_Bayes2.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduce computation tools to perform inference for simple models in R (how to turn the Bayesian crank)\nAppreciate the need for sensitivity analysis, model checking and comparison, and the potential dangers of Bayesian methods."
  },
  {
    "objectID": "VB_Bayes2.html#what-if-we-cant-calculate-an-analytic-posterior",
    "href": "VB_Bayes2.html#what-if-we-cant-calculate-an-analytic-posterior",
    "title": "VectorByte Methods Training",
    "section": "What if we can’t calculate an analytic posterior?",
    "text": "What if we can’t calculate an analytic posterior?\nIf we go back to the full Bayes theorem: \\[\n\\text{Pr}(\\theta|Y) = \\frac{\\mathcal{L}(\\theta; Y)f(\\theta)}{\\text{Pr}(Y)}\n\\] We are usually specifying the likelihood and the prior but we often don’t know the normalizing constant in the denominator. Without this, the probabilities don’t properly integrate to 1 and we can’t make probability statements.\nWe can use Monte Carlo methods to approximate the posterior."
  },
  {
    "objectID": "VB_Bayes2.html#stochastic-simulation-monte-carlo",
    "href": "VB_Bayes2.html#stochastic-simulation-monte-carlo",
    "title": "VectorByte Methods Training",
    "section": "Stochastic Simulation & Monte Carlo",
    "text": "Stochastic Simulation & Monte Carlo\nStochastic simulation is a way to understand variability in a system and for calculating quantities that may be difficult or impossible to obtain directly.\n\nMonte Carlo (MC) methods are “a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.” - Wikipedia"
  },
  {
    "objectID": "VB_Bayes2.html#mc-for-bayesian-statistics",
    "href": "VB_Bayes2.html#mc-for-bayesian-statistics",
    "title": "VectorByte Methods Training",
    "section": "MC for Bayesian Statistics",
    "text": "MC for Bayesian Statistics\nWe use Monte Carlo (MC) methods to generate random deviates in the right ratios from the target posterior called draws or samples.\n\nWe use these draws to approximate/summarize our distribution and make inference statements (point estimates, CIs, etc). We can also use the draws to calculate the posterior distribution of any function of our estimated parameters.\n\nAs the number of draws/samples gets large we can approximate these quantities arbitrarily high precision."
  },
  {
    "objectID": "VB_Bayes2.html#the-plug-in-principle",
    "href": "VB_Bayes2.html#the-plug-in-principle",
    "title": "VectorByte Methods Training",
    "section": "The “plug-in principle”",
    "text": "The “plug-in principle”\nUsing MC to perform these calculations (and to propagate the uncertainty) rests on the idea of the plug-in principle:\n\nA summary statistic or other feature of a distribution (e.g. expected value) can be approximated by the same summary/feature of an empirical sample from that distribution (e.g., sample mean)."
  },
  {
    "objectID": "VB_Bayes2.html#markov-chain-mc-mcmc",
    "href": "VB_Bayes2.html#markov-chain-mc-mcmc",
    "title": "VectorByte Methods Training",
    "section": "Markov Chain MC (MCMC)",
    "text": "Markov Chain MC (MCMC)\nMCMC is the most commonly used numerical algorithm for generating posterior samples.\n A Markov Chain is a sequence of randomly generated numbers where each draw depends on the one immediately preceding it.\n\nPlot – Ian Murray (http://mlg.eng.cam.ac.uk/zoubin/tut06/mcmc.pdf)"
  },
  {
    "objectID": "VB_Bayes2.html#gibbs-sampling",
    "href": "VB_Bayes2.html#gibbs-sampling",
    "title": "VectorByte Methods Training",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\nGibbs sampling is a type of MCMC that leverages the conditional distributions of parameters to generate samples by proposing them one at a time. This is the algorithm implemented in the popular Bayesian packages BUGS, WinBUGS, \\({\\tt nimble}\\), and JAGS/\\({\\tt rjags}\\), and that we use for \\({\\tt bayesTPC}\\).\n We will treat Gibbs sampling and other of the numerical methods as mostly “black boxes”. We’ll learn to diagnose output from these later on in the practical component."
  },
  {
    "objectID": "VB_Bayes2.html#what-do-we-do-with-posterior-samples",
    "href": "VB_Bayes2.html#what-do-we-do-with-posterior-samples",
    "title": "VectorByte Methods Training",
    "section": "What do we do with Posterior Samples?",
    "text": "What do we do with Posterior Samples?\nWe can treat the draws much like we would data:\n\nCalculate posterior summaries (mean, median, mode, etc) just like we would a data sample\nCalculate precision of the summaries (e.g., sample variance)\nCIs via quantiles (order statistics of the data) or HPD intervals (using \\({\\tt CODA}\\) package in \\({\\tt R}\\))\n\n\nIf the samples are parameters in a complex model, we can plug them all in, one at a time, to get a range of possible predictions from the model (we’ll see this in the practical bit, later on)."
  },
  {
    "objectID": "VB_Bayes2.html#models-comparison-via-dic",
    "href": "VB_Bayes2.html#models-comparison-via-dic",
    "title": "VectorByte Methods Training",
    "section": "Models Comparison via (DIC)",
    "text": "Models Comparison via (DIC)\nThe  Deviance Information Criterion (DIC) seeks to judge a model on how well it fits, penalized by the complexity of the model: \\[\nDIC = D(\\bar{\\theta}) + 2p_D\n\\] where:\n\nDeviance: \\(D(\\theta)=-2\\log(\\mathcal{L}(\\theta; y)) + C\\)\nPenalty: \\(p_D = \\bar{D} -D(\\bar{\\theta})\\)\n\\(D(\\bar{\\theta})\\): deviance at the posterior mean of \\(\\theta\\)\n\\(\\bar{D}\\): average deviance across the posterior samples.\n\n\\(\\rightarrow\\) Already implemented in nimble!"
  },
  {
    "objectID": "VB_Bayes2.html#bayesian-using-nimblejags",
    "href": "VB_Bayes2.html#bayesian-using-nimblejags",
    "title": "VectorByte Methods Training",
    "section": "Bayesian using nimble/JAGS",
    "text": "Bayesian using nimble/JAGS\nBoth nimble and JAGS implement Gibbs sampling/MCMC in a fairly easy to use package that you can call from R. Models are encoded using the BUGS language.\n\nThat is, once you specify the appropriate sampling distribution/likelihood and any priors for the parameters, it will use MCMC to obtain samples from the posterior in the right ratios so that we can calculate whatever we want."
  },
  {
    "objectID": "VB_Bayes2.html#specifying-a-bugs-model",
    "href": "VB_Bayes2.html#specifying-a-bugs-model",
    "title": "VectorByte Methods Training",
    "section": "Specifying a BUGS model",
    "text": "Specifying a BUGS model\nThe trickiest and most important part of each analysis is properly specifying the model for all of the data that you want to fit. Before you begin to code, you need to decide:\n\n\nWhat is the relationship between your predictors and your response?\nWhat kind of probability distribution should you use to describe your response variable?\nAre there any constraints on your parameters or responses that you need to encode in your prior or likelihood, respectively?"
  },
  {
    "objectID": "VB_Bayes2.html#next-steps",
    "href": "VB_Bayes2.html#next-steps",
    "title": "VectorByte Methods Training",
    "section": "Next Steps",
    "text": "Next Steps\nThere are two practicals focusing on using \\({\\tt nimble}\\) and \\({\\tt bayesTPC}\\) to conduct analyses. It has two main chunks:\n\nComparing your conjugate Bayesian analysis on the midge data to the approximate results with \\({\\tt nimble}\\).\nFitting a TPC to trait data using \\({\\tt bayesTPC}\\) (easier than having to code it yourself in \\({\\tt nimble}\\)!).\n\nFor both you’ll be led through visualizing your MCMC chains and your posterior distributions of parameters and predictions. There are also advanced practice suggestions for those who want to go further."
  },
  {
    "objectID": "bayesTPC_activity.html",
    "href": "bayesTPC_activity.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "This section is focused on using the bayesTPC package to fit TPCs to data using the methods we’ve explored in the Bayesian lectures and the first two activities. Here we won’t be talking much about the implementation, but instead will rely on the bayesTPC package and it’s functions to allow us to specify, fit, and analyze the data."
  },
  {
    "objectID": "bayesTPC_activity.html#data-from-vectraits",
    "href": "bayesTPC_activity.html#data-from-vectraits",
    "title": "Introduction to Bayesian Methods",
    "section": "Data from VecTraits",
    "text": "Data from VecTraits\nThe data we want to fit is available on the VecTraits database. We can use the helper function included as part of bayesTPC that isdesigned to interact with this database, get_datasets(), along with the appropriate data set id numbers to retrieve and load them into R.\n\naedes_data &lt;- get_datasets(577:579)\n\nWe have downloaded all three datasets. As always, first we have a look at the data (just looking at a few columns, since the VecTraits format is large so it can hold a lot of different types of information):\n\ncols&lt;-c(2,5,6,9,68)\naedes1&lt;-aedes_data[[3]]\nhead(aedes1[,cols])\n\n  DatasetID OriginalTraitName                        OriginalTraitDef\n1       579         longevity individual-level duration of life stage\n2       579         longevity individual-level duration of life stage\n3       579         longevity individual-level duration of life stage\n4       579         longevity individual-level duration of life stage\n5       579         longevity individual-level duration of life stage\n6       579         longevity individual-level duration of life stage\n  OriginalTraitValue Interactor1Temp\n1                 10              22\n2                  9              22\n3                  7              22\n4                 11              22\n5                 10              22\n6                  6              22\n\n\nTrait data need to be in a particular format before being passed into the fitting routine. Specifically, the data must be stored as a list with names Trait for the modeled response and Temp for the corresponding temperature settings (in ^\\circC, as this is necessary for some of the TPC functions). We format the three datasets here:\n\n# development rate\ndev_rate &lt;- list(Trait = 1/aedes_data[[2]]$OriginalTraitValue,\n                 Temp = aedes_data[[2]]$Interactor1Temp)\n\n# adult longevity\nadult_life &lt;- list(Trait = aedes_data[[3]]$OriginalTraitValue,\n                   Temp = aedes_data[[3]]$Interactor1Temp)\n\n# juvenile survival data\njuv_survival &lt;- list(Trait = aedes_data[[1]]$OriginalTraitValue,\n                     Temp = aedes_data[[1]]$Interactor1Temp)\n\nNotice that we follow a convention here treating the development rate as 1/development time. This is a common assumption (although it is formally only valid if we believe that development times are exponentially distributed). Often mathematical models assume exponentially distributed traits, and so this is why the data are modeled in this fashion. We will show this approach here, as it is common, but do not advocate for this in general.\n\n\n\n\n\n\n\n\nFigure 1: Three example datasets from @huxley2022competition: Development Rate, Adult Longevity, and Juvenile Survival. Temperatures for the juvenile survival data are jittered for visibility\n\n\n\n\n\nWe will first go through a case where the default settings give reasonable output out of the box (adult lifespan) in order to show basic functions in action. We then approach a case where the defaults need to be modified (development rate). As part of your independent practice, you can fit data (juvenile survival) where we would use a glm model for the data."
  },
  {
    "objectID": "bayesTPC_activity.html#inference-with-default-settings",
    "href": "bayesTPC_activity.html#inference-with-default-settings",
    "title": "Introduction to Bayesian Methods",
    "section": "Inference with default settings",
    "text": "Inference with default settings\nOnce we have the data formatted as required by b_TPC(), we can fit each of the datasets with a single call, using the default settings. As we saw in the plot above, adult lifespan are numeric data where a concave down unimodal response is likely appropriate. For adult lifespan, we chose to fit a Briere function with the default specification:\n\nget_default_model_specification(\"briere\")\n\nbayesTPC Model Specification of Type:\n  briere\n\nModel Formula:\n  m[i] &lt;- ( q * Temp * (Temp - T_min) * sqrt((T_max &gt; Temp) * abs(T_max - Temp))\n* (T_max &gt; Temp) * (Temp &gt; T_min) )\n\nModel Distribution:\nTrait[i] ~ T(dnorm(mean = m[i], tau = 1/sigma.sq), 0, )\n\nModel Parameters and Priors:\n  q ~ dunif(0, 1) \n  T_max ~ dunif(25, 60) \n  T_min ~ dunif(0, 24)\n\nPrior for Variance:\n  sigma.sq ~ dexp(1)\n\n\nTo fit the model then requires a single line of code with the first argument being the name of the formatted data object and the second being the name of the TPC that we want to use for fitting. By default we take 10000 samples, no burn-in, using a random walk sampler.\n\nadult_life_fit &lt;- b_TPC(adult_life, \"briere\")\n\nOnce the fitting process has completed (which can take a few minutes), we can have a gander at the fitted model object using print. This command provides details about the model fit, the priors, and some simple summaries of the fitted model.\n\nprint(adult_life_fit)\n\nbayesTPC MCMC of Type:\n  briere\n\nFormula:\n  m[i] &lt;- ( q * Temp * (Temp - T_min) * sqrt((T_max &gt; Temp) * abs(T_max - Temp))\n* (T_max &gt; Temp) * (Temp &gt; T_min) )\n\nDistribution:\nTrait[i] ~ T(dnorm(mean = m[i], tau = 1/sigma.sq), 0, )\n\nParameters:\n            MAP   Mean Median        Priors\nT_max    26.353 26.152 26.079 dunif(25, 60)\nT_min    19.482 20.776 21.090  dunif(0, 24)\nq         0.072  0.295  0.204   dunif(0, 1)\nsigma.sq 38.915 39.065 39.148       dexp(1)\n\n\nWe can also see what is in the object:\n\nnames(adult_life_fit)\n\n[1] \"samples\"        \"mcmc\"           \"data\"           \"model_spec\"    \n[5] \"priors\"         \"constants\"      \"uncomp_model\"   \"comp_model\"    \n[9] \"MAP_parameters\"\n\n\nMost of what we do relies on the “samples” portion of the object, although bayesTPC has helper functions to reduce the need to interact with these directly in most cases."
  },
  {
    "objectID": "bayesTPC_activity.html#mcmc-diagnotic-plots",
    "href": "bayesTPC_activity.html#mcmc-diagnotic-plots",
    "title": "Introduction to Bayesian Methods",
    "section": "MCMC Diagnotic Plots",
    "text": "MCMC Diagnotic Plots\nb_TPC() returns an object of class btpc_MCMC which contains (along with model specification information and data) the MCMC samples as an mcmc object from the package coda. As mentioned in the previous portion of the training, it is important to check the MCMC traceplot before using or interpreting a fitted model to ensure the chains have converged. An MCMC traceplot shows each sample for a parameter in the order that the samples were taken. If the model has converged, the traceplot will eventually start varying around a single point, resembling a “fuzzy caterpillar”.\n\npar(mfrow=c(2,2), mar=c(4,3,3,1)+.1)\ntraceplot(adult_life_fit)\n\n\n\n\n\n\n\nFigure 2: Traceplots for the three parameters of the Briere TPC and the observation parameter for model fitted to the Adult Longevity data.\n\n\n\n\n\nWe notice that it takes a while for the chains to converge. Thus we need to specify a burn-in period and only consider samples obtained after the burn-in. For this example, a burn-in of around 3000 should give us a good result. We can re-visualize with this burn-in (by adding burn=3000 as an argument to traceplot):\n\npar(mfrow=c(2,2), mar=c(4,3,3,1)+.1)\nmyburn&lt;-3000\ntraceplot(adult_life_fit, burn=myburn)\n\n\n\n\n\n\n\nFigure 3: Traceplots for the three parameters of the Briere TPC and the observation parameter for model fitted to the Adult Longevity data. Here the burn-in portion of the MCMC chains has been dropped.\n\n\n\n\n\nThis is much better! All of the chains now have the desired “fuzzy caterpillar” look. Typically one would at this point go back to the original fitting function, and specify the burn-in time, along with potentially increasing the total sample size in order to ensure sufficient samples. This approach drops the burnin samples from the returned object. For brevity herewe will simply specify the value of burn as an argument for the remaining plotting functions.\nWe can examine the ACF of the chains as well (one for each parameter), similarly to a time series, to again check for autocorrelation within the chain (we want the autocorrelation to be fairly low):\n\ns1&lt;-as.data.frame(adult_life_fit$samples[myburn:10000,])\npar(mfrow=c(2,2), bty=\"n\", mar=c(4,4,3,1)+.1)\nfor(i in 1:4) {\n  acf(s1[,i], lag.max=50, main=\"\",\n      ylab = paste(\"ACF: \", names(s1)[i], sep=\"\"))\n}\n\n\n\n\n\n\n\n\nThere is still autocorrelation, especially for two of the quadratic parameters. The chain for \\sigma is mixing best (the ACF falls off the most quickly). We could reduce the autocorrelation even further by thinning the chain (i.e., change the nt parameter to 5 or 10), or changing the type of sampler.\nA second important diagnostic step is to compare the marginal priors and posteriors of our model parameters. This enables us to confirm that (unless we’ve purposefully specified an informative prior) that our posterior distributions have been informed by the data. bayesTPC includes a built in function, ppo_plot(), that creates posterior/prior overlap plots for all model parameters (note that the priors are smoothed because the algorithm uses kernel smoothing instead of the exact distribution).\n\npar(mfrow=c(2,2), mar=c(4,3,3,1)+.1)\nppo_plot(adult_life_fit, burn=myburn, legend_position = \"topright\")\n\n\n\n\n\n\n\nFigure 4: Marginal prior/posterior for the three parameters of the Briere TPC and the observation parameter for the model fitted to the Adult Longevity data. Note that the burn-in is dropped for these plots using the burn argument.\n\n\n\n\n\nThe prior distribution here is very different from the posterior. These data are highly informative for the parameters of interest and are very unlikely to be influenced much by the prior distribution (although you can always change the priors to check this). However, notice that the posterior T_0 is slightly truncated by their priors. If priors and posteriors are very similar one should shift the priors, and re-run."
  },
  {
    "objectID": "bayesTPC_activity.html#additional-plotting",
    "href": "bayesTPC_activity.html#additional-plotting",
    "title": "Introduction to Bayesian Methods",
    "section": "Additional plotting",
    "text": "Additional plotting\nAfter we have established appropriate burn-in values, we can use plot(), posterior_predictive(), and plot_prediction() to examine the fit of the model in two ways. The defaults for the plot() function plots the median and 95% Highest Posterior Density (HPD) interval of the fitted function (i.e., plugging the samples into the TPC function, and calculating the median and HPD interval at all evaluated temperatures). In contrast, the posterior_predictive() function uses simulation to draw points from the posterior predictive distribution, and so it includes both the samples describing the TPC function and the observational model. It then uses these samples to calculate the mean/median and the HPD interval of those simulated points. Both kinds of plots are shown in Figure 5 for comparison.\n\n\n\n\n\n\n\n\nFigure 5: Comparison of the plots produced by the plot() function (LEFT) and the combination of the posterior_predictive(), and plot_prediction() functions (RIGHT). By default both functions would only make plots/predictions within the range of temperatures included in the fitted data. However here we show the use of the temp_interval argument to enable plotting of predictions across a broader temperature range.\n\n\n\n\n\nNote that the two bottom fits show different output. On the left we show the HPD bounds and median of the fitted Briere function, only. On the right, in contrast, shows the bounds and mean/median of the posterior predictive distribution (so it includes the randomness that is part of the truncated normal observation model). Notice that the HPD predictive interval is a bit jagged here. These intervals are generated using sampling from the posterior predictive distribution. Increasing the total number of samples can give smoother bounds in general.\nNow that we’ve confirmed that things are working well, it’s often useful to also look at the joint distribution of all of your parameters together to understand how estimates are related to each other. Of course, if you have a high dimensional posterior, rendering a 2-D representation can be difficult. The standard is to examine the pair-wise posterior distribution. We can do this using the function bayesTPC_ipairs():\n\nbayesTPC_ipairs(adult_life_fit, burn=myburn)\n\n\n\n\n\n\n\nFigure 6: Pairwise visualization of the joint posterior distribution of parameters for the three parameters of the Briere TPC and the observation parameter for the model fitted to the Adult Longevity data. Note that the burn-in is dropped for this plot using the burn argument.\n\n\n\n\n\nNotice that there is substantial correlation between q and T_{min} (a.k.a., T_0). This is typical for most TPCs, and is one of the reasons why prior choice can be very important!"
  },
  {
    "objectID": "bayesTPC_activity.html#summaries",
    "href": "bayesTPC_activity.html#summaries",
    "title": "Introduction to Bayesian Methods",
    "section": "Summaries",
    "text": "Summaries\nIf we are satisfied with the traceplots, fits, and prior/posterior plots, users may want to examine additional summary output (for example to make tables) or to save summaries. Numerical summaries are available through the print() function shown above, but we can see more details using summary().\n\nsummary(adult_life_fit, burn=myburn)\n\nbayesTPC MCMC of Type:\n  briere\n\nFormula:\n  m[i] &lt;- ( q * Temp * (Temp - T_min) * sqrt((T_max &gt; Temp) * abs(T_max - Temp))\n* (T_max &gt; Temp) * (Temp &gt; T_min) )\n\nDistribution:\nTrait[i] ~ T(dnorm(mean = m[i], tau = 1/sigma.sq), 0, )\n\nPriors:\n  q ~ dunif(0, 1) \n  T_max ~ dunif(25, 60) \n  T_min ~ dunif(0, 24) \n  sigma.sq ~ dexp(1)\n\nMax. A Post. Parameters: \n     T_max      T_min          q   sigma.sq   log_prob \n   26.3533    19.4824     0.0721    38.9154 -2039.2996 \n\nMCMC Results:\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean     SD Naive SE Time-series SE\nT_max    26.1524 0.2636 0.002636        0.02564\nT_min    20.7761 1.0195 0.010195        0.36384\nq         0.2954 0.2313 0.002313        0.10052\nsigma.sq 39.0648 3.0974 0.030974        0.21892\n\n2. Quantiles for each variable:\n\n             2.5%     25%     50%    75%   97.5%\nT_max    26.00595 26.0197 26.0786 26.196 26.6038\nT_min    18.34540 20.3727 21.0898 21.577 21.7813\nq         0.04796  0.1139  0.2043  0.446  0.8838\nsigma.sq 34.82566 37.7241 39.1485 40.724 43.6478\n\n\nWe may want to store some of the sample statistics for later use. The sample-based Maximum a posteriori (MAP) estimator is calculated and saved as part of our fitting process, and can be obtained directly from the fit object.\n\n# MAP estimator\nadult_life_fit$MAP_parameters |&gt; round(5)\n\n      T_max       T_min           q    sigma.sq    log_prob \n   26.35330    19.48236     0.07213    38.91544 -2039.29956 \n\n\nThe other sample statistics for each parameter can be obtained by saving the summary of the samples in the fitted model object. Then the various statistics may be extracted directly:\n\nestimates &lt;- summary(adult_life_fit$samples)\n\n# Mean\nestimates$statistics[,\"Mean\"]\n\n     T_max      T_min          q   sigma.sq \n26.1523797 20.7760824  0.2953669 39.0648293 \n\n# Median and 95% CI bounds\nestimates$quantiles[,c(\"2.5%\", \"50%\", \"97.5%\")]\n\n                2.5%        50%      97.5%\nT_max    26.00594550 26.0785582 26.6038477\nT_min    18.34539630 21.0897987 21.7813345\nq         0.04796055  0.2042684  0.8837863\nsigma.sq 34.82565698 39.1484559 43.6478472\n\n\nIf one instead would like the Highest Posterior Density bounds (instead of the quantile based summaries) the HPDinterval function from coda may be used.\n\nHPDinterval(adult_life_fit$samples)\n\n               lower     upper\nT_max    26.00327163 26.541295\nT_min    18.62211018 21.816110\nq         0.04247238  0.762813\nsigma.sq 34.82383141 43.592328\nattr(,\"Probability\")\n[1] 0.95"
  },
  {
    "objectID": "bayesTPC_activity.html#fitting-with-default-settings",
    "href": "bayesTPC_activity.html#fitting-with-default-settings",
    "title": "Introduction to Bayesian Methods",
    "section": "Fitting with default settings",
    "text": "Fitting with default settings\nThe default priors chosen in bayesTPC are generally as non-restrictive as possible, which can lead to inappropriate fits when the response trait is very close to zero. In this case it may be necessary to modify the priors. It is also possible that mixing may be poor with the default sampler, and so we may need to update the sampling method. We briefly show these issues using the development rate data as an example. These data are also numeric and we assume, again, a Briere functional response:\n\ndev_rate_fit &lt;- b_TPC(dev_rate, \"briere\")\n\nHowever in this case, although the traceplots look ok (after a burnin) the fits are very poor (Figure 7)\n\n\n\n\n\n\n\n\nFigure 7: Traceplots for the three parameters of the Briere TPC and the observation parameter for model fitted to the juvenile development data. (top 4 panels) with the data (bottom left) and corresponding posterior predictive (bottom right) plots based on these samples. The burn-in portion of the MCMC chains has been dropped.\n\n\n\n\n\nWe can see a bit of what is going on by looking at the marginal prior/posterior plots:\n\npar(mfrow=c(2,2), mar=c(4,3,3,1)+.1)\nppo_plot(dev_rate_fit, burn=myburn, legend_position = \"topright\")\n\n\n\n\n\n\n\nFigure 8: Marginal prior/posterior for the three parameters of the Briere TPC and the observation parameter for the model fitted to the Juvenile development rate data. Note that the burn-in is dropped for these plots using the burn argument.\n\n\n\n\n\nHere the posterior for q is effectively the same as the prior, and both T_{min} and T_{max} are bumping up against the edges of their ranges."
  },
  {
    "objectID": "bayesTPC_activity.html#changing-fitting-arguments",
    "href": "bayesTPC_activity.html#changing-fitting-arguments",
    "title": "Introduction to Bayesian Methods",
    "section": "Changing Fitting arguments",
    "text": "Changing Fitting arguments\nIn order to try to improve the fitting, we will modify the priors and the sampler. We notice from the plot of the data that the rate seems to increase across the observed range of the data. Thus it is likely that the T_{\\mathrm{max}} parameter is at or above the temperature manipulated in the experiment and T_{\\mathrm{min}} is below. We can pass this information in as new priors through the priors argument as shown. Further, we can switch to a sampler that has a better chance of converging by modifying the samplerType parameter. Here, we use automated factor slice sampling. Although this sampler is often more effective (especially when parameters are highly correlated, as they are for most TPCs), it is slower and so is not used by default. These changes result in much better fits (see Figure 9).\n\ndev_rate_fit2 &lt;- b_TPC(dev_rate, \"briere\",\n                       priors = list(T_min = \"dunif(0,22)\",\n                                     T_max = \"dunif(34,50)\"),\n                       burn=myburn,\n                       samplerType = \"AF_slice\")\n\n\n\n\n\n\n\n\n\nFigure 9: Traceplots for the three parameters of the Briere TPC and the observation parameter for the model fitted to the juvenile development data (top 4 panels) with the corresponding summary (bottom left) and posterior predictive (bottom right) plots based on these samples. The burn-in portion of the MCMC chains has been dropped as part of the fitting.\n\n\n\n\n\nThis is much better! The chains mix well, and our predictions now lie along with the data. If you were to plot the autocorrelation you would notice that it falls off much more quickly."
  },
  {
    "objectID": "bayesTPC_activity.html#model-selection",
    "href": "bayesTPC_activity.html#model-selection",
    "title": "Introduction to Bayesian Methods",
    "section": "Model Selection",
    "text": "Model Selection\nWhat if we didn’t know that the Briere was the preferred model for these development rate data? We can fit with another function, check all of the diagnostics, and then compare via WAIC (Widely Applicable Information Criterion) to choose between them. For the juvenile development rate, let’s try a quadratic. Let’s look at the implementation details:\n\nget_formula(\"quadratic\")\n\nexpression(-1 * q * (Temp - T_min) * (Temp - T_max) * (T_max &gt; \n    Temp) * (Temp &gt; T_min))\n\n\n\nget_default_priors(\"quadratic\")\n\n              q           T_max           T_min \n  \"dunif(0, 1)\" \"dunif(25, 60)\"  \"dunif(0, 24)\" \n\n\nI will refit using the default priors, except for the prior for q (so you can see an option):\n\ndev_rate_fit_Quad &lt;- b_TPC(data = dev_rate, ## data\n                           model = 'quadratic', ## model to fit\n                           niter = 10000, ## total iterations\n                           burn = 3000, ## number of burn in samples\n                           samplerType = 'AF_slice', ## slice sampler\n                           priors = list(q = 'dexp(1)') ## priors\n                    ) \n\nCreating NIMBLE model:\n - Configuring model.\n - Compiling model.\n\nCreating MCMC:\n - Configuring MCMC.\n - Compiling MCMC.\n - Running MCMC.\n\nProgress:\n|-------------|-------------|-------------|-------------|\n|-------------------------------------------------------|\n\nConfiguring Output:\n - Finding Max. a Post. parameters.\n\n\n\nsummary(dev_rate_fit_Quad)\n\nbayesTPC MCMC of Type:\n  quadratic\n\nFormula:\n  m[i] &lt;- ( -1 * q * (Temp - T_min) * (Temp - T_max) * (T_max &gt; Temp) * (Temp &gt;\nT_min) )\n\nDistribution:\nTrait[i] ~ T(dnorm(mean = m[i], tau = 1/sigma.sq), 0, )\n\nPriors:\n  q ~ dexp(1) \n  T_max ~ dunif(25, 60) \n  T_min ~ dunif(0, 24) \n  sigma.sq ~ dexp(1)\n\nMax. A Post. Parameters: \n    T_max     T_min         q  sigma.sq  log_prob \n  56.1306   16.1473    0.0004    0.0003 1855.7780 \n\nMCMC Results:\nIterations = 1:7000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 7000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean        SD  Naive SE Time-series SE\nT_max    5.619e+01 1.960e+00 2.342e-02      7.562e-02\nT_min    1.616e+01 3.453e-01 4.127e-03      1.272e-02\nq        3.590e-04 3.728e-05 4.455e-07      1.470e-06\nsigma.sq 2.865e-04 1.543e-05 1.844e-07      1.978e-07\n\n2. Quantiles for each variable:\n\n              2.5%       25%       50%       75%     97.5%\nT_max    5.253e+01 5.476e+01 5.612e+01 5.769e+01 5.972e+01\nT_min    1.551e+01 1.591e+01 1.616e+01 1.641e+01 1.681e+01\nq        2.990e-04 3.298e-04 3.569e-04 3.842e-04 4.374e-04\nsigma.sq 2.579e-04 2.757e-04 2.860e-04 2.967e-04 3.185e-04\n\n\nWe again plot the chains of the three main TPC parameters and the standard deviation of the normal observation model:\n\n\n\n\n\n\n\n\nFigure 10: Traceplots for the three parameters of the quadratic TPC and the observation parameter for the model fitted to the juvenile development data (top 4 panels) with the corresponding summary (bottom left) and posterior predictive (bottom right) plots based on these samples. The burn-in portion of the MCMC chains has been dropped.\n\n\n\n\n\nOnce fitted, we want to be able to choose which of these models is best for these data. Although, a priori we would expect the Briere fit to be best development rates, both fits seem visually reasonable. We can extract the the values of the wAIC [@watanabe2009algebraic] to compare the performance of our models using get_WAIC() (which wraps nimble’s getWAIC() function and produces a tidier format). The preferred model will be the one with the lowest wAIC value. Note that because this is based on samples, you want to have the same number of samples in the chains for each of the models you are comparing.\n\nbayesTPC::get_WAIC(dev_rate_fit2)\n\n        WAIC         lppd        pWAIC \n-3731.677426  1870.397519     4.558806 \n\nbayesTPC::get_WAIC(dev_rate_fit_Quad)\n\n        WAIC         lppd        pWAIC \n-3717.368134  1862.632830     3.948763 \n\n\nIn this case the WAIC for the Briere fit is more negative and so is the preferred model."
  },
  {
    "objectID": "bayesTPC_activity.html#practice-option-1",
    "href": "bayesTPC_activity.html#practice-option-1",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice Option 1",
    "text": "Practice Option 1\nFor the Aedes aegypti life span data, try to fit 3 TPC functional forms (Briere, quadratic, and Stinner) to the data. Use slice sampling for all three of them, and make the chains a bit longer (say 15000 plus the 3000 burnin). Check all of the diagnostics for all models, and plot the fits. Then compare the three models via WAIC. Which one comes out on top."
  },
  {
    "objectID": "bayesTPC_activity.html#practice-option-2",
    "href": "bayesTPC_activity.html#practice-option-2",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice Option 2",
    "text": "Practice Option 2\nYou can download some other trait data from the VectorByte – VecTraits Databases or use your own data. Write you own analysis as an independent, self-sufficient R script that produces all the plots in a reproducible workflow when sourced. Use the appropriate functional forms for your data."
  },
  {
    "objectID": "bayesTPC_activity.html#practice-option-3",
    "href": "bayesTPC_activity.html#practice-option-3",
    "title": "Introduction to Bayesian Methods",
    "section": "Practice Option 3",
    "text": "Practice Option 3\nIn addition to the two datasets that we explored here, we downloaded data on juvenile survival. These data are encoded as zeros and ones, and are more appropriately modeled using a Bernoulli/Binomial distribution – that is as GLMs. bayesTPC has implemented two options for fitting these models, either a linear or quadratic:\n\n## Linear:\nget_default_model_specification(\"bernoulli_glm_lin\")\n\nbayesTPC Model Specification of Type:\n  bernoulli_glm_lin\n\nModel Formula:\n  logit(m[i]) &lt;- ( B0 + B1 * Temp )\n\nModel Distribution:\nTrait[i] ~ dbern(m[i])\n\nModel Parameters and Priors:\n  B0 ~ dnorm(0, 500) \n  B1 ~ dnorm(0, 500)\n\n\n\n## Quadratic:\nget_default_model_specification(\"bernoulli_glm_quad\")\n\nbayesTPC Model Specification of Type:\n  bernoulli_glm_quad\n\nModel Formula:\n  logit(m[i]) &lt;- ( B0 + B1 * Temp + B2 * (Temp)^2 )\n\nModel Distribution:\nTrait[i] ~ dbern(m[i])\n\nModel Parameters and Priors:\n  B0 ~ dnorm(0, 500) \n  B1 ~ dnorm(0, 500) \n  B2 ~ dnorm(0, 500)\n\n\nFit the juvenile survival data using both of these models. Note that the slice sampler is typically better for these, and you will likely need a longer chain and more burn-in. Make sure to use the same number of samples/burn-in for both models. Check all the diagnostic plots and plot the data with the fits. Then use WAIC to choose between the two model variants. What do you conclude?"
  }
]